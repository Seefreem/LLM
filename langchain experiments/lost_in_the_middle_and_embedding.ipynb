{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seelur/enter/envs/langchain/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/seelur/enter/envs/langchain/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='The Celtics are my favourite team.'),\n",
       " Document(page_content='L. Kornet is one of the best Celtics players.'),\n",
       " Document(page_content='The Boston Celtics won the game by 20 points'),\n",
       " Document(page_content='Larry Bird was an iconic NBA player.'),\n",
       " Document(page_content='Elden Ring is one of the best games in the last 15 years.'),\n",
       " Document(page_content='Basquetball is a great sport.'),\n",
       " Document(page_content='I simply love going to the movies'),\n",
       " Document(page_content='Fly me to the moon is one of my favourite songs.'),\n",
       " Document(page_content='This is just a random text.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Get embeddings.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# 注意embedding于使用的模型无关，embedding只影响到 retrieval 阶段，也就是计算相似度阶段\n",
    "\n",
    "texts = [\n",
    "    \"Basquetball is a great sport.\",\n",
    "    \"Fly me to the moon is one of my favourite songs.\",\n",
    "    \"The Celtics are my favourite team.\",\n",
    "    \"This is a document about the Boston Celtics\",\n",
    "    \"I simply love going to the movies\",\n",
    "    \"The Boston Celtics won the game by 20 points\",\n",
    "    \"This is just a random text.\",\n",
    "    \"Elden Ring is one of the best games in the last 15 years.\",\n",
    "    \"L. Kornet is one of the best Celtics players.\",\n",
    "    \"Larry Bird was an iconic NBA player.\",\n",
    "]\n",
    "\n",
    "# Create a retriever\n",
    "retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ") # 这个参数指的是返回的条目数量。注意这里存在一个缓存，就是如果你改小了这个参数，那么后面你获得的结果就很有限了。你需要重启vscode才行。\n",
    "query = \"What can you tell me about the Celtics?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The Celtics are my favourite team.'),\n",
       " Document(page_content='The Boston Celtics won the game by 20 points'),\n",
       " Document(page_content='Elden Ring is one of the best games in the last 15 years.'),\n",
       " Document(page_content='I simply love going to the movies'),\n",
       " Document(page_content='This is just a random text.'),\n",
       " Document(page_content='Fly me to the moon is one of my favourite songs.'),\n",
       " Document(page_content='Basquetball is a great sport.'),\n",
       " Document(page_content='Larry Bird was an iconic NBA player.'),\n",
       " Document(page_content='L. Kornet is one of the best Celtics players.'),\n",
       " Document(page_content='This is a document about the Boston Celtics')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder the documents:\n",
    "# Less relevant document will be at the middle of the list and more\n",
    "# relevant elements at beginning / end.\n",
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "# Confirm that the 4 relevant documents are at beginning and end.\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Celtics are a professional basketball team based in Boston, Massachusetts. They have won several championships and have some of the most iconic players in NBA history, including Larry Bird. They recently won a game by 20 points, and they have a player named L. Kornet who is considered one of the best Celtics players.\n"
     ]
    }
   ],
   "source": [
    "# We prepare and run a custom Stuff chain with reordered docs as context.\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "# Override prompts\n",
    "# This is the template for formatting each document\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "# This is the name for replacing documents in the input prompt\n",
    "document_variable_name = \"context\"\n",
    "\n",
    "stuff_prompt_override = \"\"\"Given this text extracts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\"\"\"\n",
    "# This is the input prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=stuff_prompt_override, input_variables=[\"context\", \"query\"]\n",
    ")\n",
    "\n",
    "# Instantiate the chain\n",
    "llm = OpenAI() # 获取LLM\n",
    "# llm_chain = LLMChain(llm=llm, prompt=prompt) # 这是个run LLM (不是ChatModel)的链\n",
    "'''\n",
    "This chain takes a list of documents and first combines them into a single string. \n",
    "It does this by formatting each document into a string with the document_prompt \n",
    "and then joining them together with document_separator. \n",
    "It then adds that new string to the inputs with the variable name set by document_variable_name. \n",
    "Those inputs are then passed to the llm_chain.\n",
    "也就是说这里在插入document的时候，首先是将单个document 用document_prompt格式化成字符串。\n",
    "然后通过 document_variable_name 指定的name 将documents 插入到给LLM的prompt中。\n",
    "'''\n",
    "# chain = StuffDocumentsChain(\n",
    "#     llm_chain=llm_chain,\n",
    "#     document_prompt=document_prompt,\n",
    "#     document_variable_name=document_variable_name,\n",
    "# )\n",
    "# chain.run(input_documents=reordered_docs, query=query)\n",
    "# chain.invoke({'input_documents':reordered_docs, 'query':query})\n",
    "\n",
    "chain =  prompt | llm\n",
    "print(chain.invoke({\"context\":reordered_docs, \"query\":query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "# 这个embedding和word embedding还不一样，因为这个是将整个句子embedding，而不是将一个单词embedding。\n",
    "# 所以无论句子长短，embedding的结果都是一样长的向量。那么就好直接计算相似度了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1536 <class 'float'>\n",
      "<class 'list'> 1536 <class 'float'>\n",
      "<class 'list'> 1536 <class 'float'>\n",
      "<class 'list'> 1536 <class 'float'>\n",
      "<class 'list'> 1536 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "for e in embeddings:\n",
    "    print(type(e), len(e), type(e[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0053546813655943075,\n",
       " -0.0005715346531097275,\n",
       " 0.038875909934336914,\n",
       " -0.0029596003572924623,\n",
       " -0.008966285328704282]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.7707570228601861\n",
      "similarity: 0.7853193706420822\n",
      "similarity: 0.8355098705084145\n",
      "similarity: 0.7740705554272965\n",
      "similarity: 0.7561473238913311\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "for e in embeddings:\n",
    "    doc = np.array(e)\n",
    "    que = np.array(embedded_query)\n",
    "    print('similarity:', 1 - scipy.spatial.distance.cosine(doc, que))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "underlying_embeddings = OpenAIEmbeddings()\n",
    "# 创建本地文件来存储cash\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "# 创建embedder\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
    ")\n",
    "\n",
    "# 加载文件。源文件中每一行是一个或者两个句子。这估计就是传统NLP的工作了吧。\n",
    "raw_documents = TextLoader(\"state_of_the_union.txt\").load()\n",
    "# 这将句子组合起来，但是保持在1000个字符以内\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 296 ms, sys: 7.64 ms, total: 304 ms\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 进行embedding 并且存储在文件中\n",
    "db = FAISS.from_documents(documents, cached_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text-embedding-ada-002704c76af-3696-5383-9858-6585616669ef',\n",
       " 'text-embedding-ada-00281426526-23fe-58be-9e84-6c7c72c8ca9a',\n",
       " 'text-embedding-ada-002abeef673-2b2a-5614-b612-d4ff3ef54c23',\n",
       " 'text-embedding-ada-0023f7b9f1f-79ae-55e3-966a-d0ec952476ed',\n",
       " 'text-embedding-ada-002a5ef11e4-0474-5725-8d80-81c91943b37f']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(store.yield_keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings \n",
    "from langchain.storage import InMemoryByteStore\n",
    "# 这种方式创建的vector store只存在于内存中，不存在于磁盘上。\n",
    "store = InMemoryByteStore()\n",
    "# 这里有个奇怪的操作，那就是将store包装到embedder中\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
