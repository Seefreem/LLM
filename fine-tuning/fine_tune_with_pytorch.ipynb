{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://pytorch.org/vision/stable/models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/seelur/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100.0%\n",
      "/home/seelur/enter/envs/pytorch_2_1_0/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow tie: 13.3%\n"
     ]
    }
   ],
   "source": [
    "# pretrained classification model -- evaluation\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "img = read_image(\"grace_hopper_517x606.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "model.eval() # Set as evaluation mode\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "13.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a classification model\n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "# Load Data\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sneaker  Shirt  Pullover  Shirt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3344, 0.0973, 0.2701, 0.2741, 0.2735, 0.5176, 0.7926, 0.5922, 0.9269,\n",
      "         0.0713],\n",
      "        [0.8182, 0.5442, 0.5341, 0.7204, 0.4201, 0.6759, 0.6634, 0.4147, 0.7528,\n",
      "         0.5426],\n",
      "        [0.7486, 0.4249, 0.7566, 0.5779, 0.6052, 0.4006, 0.3117, 0.6885, 0.4248,\n",
      "         0.7341],\n",
      "        [0.5966, 0.1874, 0.2704, 0.6450, 0.1681, 0.0293, 0.3894, 0.3291, 0.9572,\n",
      "         0.9811]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.4215619564056396\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GarmentClassifier(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizer and Device\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One epoch training function\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        # inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.2584935246729292\n",
      "  batch 2000 loss: 0.263098030246163\n",
      "  batch 3000 loss: 0.28541786968830274\n",
      "  batch 4000 loss: 0.2692911553254198\n",
      "  batch 5000 loss: 0.274459030842896\n",
      "  batch 6000 loss: 0.24538548043605624\n",
      "  batch 7000 loss: 0.27466563302189023\n",
      "  batch 8000 loss: 0.2678502871315941\n",
      "  batch 9000 loss: 0.2716107964725525\n",
      "  batch 10000 loss: 0.2659399677941765\n",
      "  batch 11000 loss: 0.2608706547279517\n",
      "  batch 12000 loss: 0.26606947224344\n",
      "  batch 13000 loss: 0.2950893120394885\n",
      "  batch 14000 loss: 0.24505291325616418\n",
      "  batch 15000 loss: 0.25576372669015107\n",
      "LOSS train 0.25576372669015107 valid 0.30427655577659607\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.25556116044890176\n",
      "  batch 2000 loss: 0.2505672002190458\n",
      "  batch 3000 loss: 0.24562027255017368\n",
      "  batch 4000 loss: 0.24546472472361894\n",
      "  batch 5000 loss: 0.26251370064943147\n",
      "  batch 6000 loss: 0.2532176042166316\n",
      "  batch 7000 loss: 0.2561687336067662\n",
      "  batch 8000 loss: 0.23847096534394666\n",
      "  batch 9000 loss: 0.2630248695426526\n",
      "  batch 10000 loss: 0.2476153080040658\n",
      "  batch 11000 loss: 0.2596690471512993\n",
      "  batch 12000 loss: 0.27540035375015576\n",
      "  batch 13000 loss: 0.26356996104369274\n",
      "  batch 14000 loss: 0.25241860577668196\n",
      "  batch 15000 loss: 0.261211242690242\n",
      "LOSS train 0.261211242690242 valid 0.2985515594482422\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "saved_model = GarmentClassifier()\n",
    "saved_model.load_state_dict(torch.load('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "# Again, grab a single mini-batch of images\n",
    "dataiter = iter(training_loader)\n",
    "next_batch = next(dataiter)\n",
    "images, labels = next_batch[0].to(device), next_batch[1].to(device)\n",
    "\n",
    "# add_graph() will trace the sample input through your model,\n",
    "# and render it as a graph.\n",
    "writer.add_graph(model, images)\n",
    "writer.flush()\n",
    "\n",
    "# To view, start TensorBoard on the command line with:\n",
    "#   tensorboard --logdir=runs\n",
    "# ...and open a browser tab to http://localhost:6006/\n",
    "# 注意上面的'runs'是日志保存的路径，在前面的代码中有设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other information\n",
    "- Using TensorBoard to visualize training progress and other activities\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
