{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seelur/enter/envs/pytorch_2_1_0/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside GenerationMinin:generation_mode=: GenerationMode.BEAM_SEARCH\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n",
      "outputs.logits.shape= torch.Size([2, 1, 50264])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. PG&E'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "torch.cuda.empty_cache() # 清空GPU缓存，清空CUDA，清空CUDA内存\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = (\n",
    "    \"Summarize the following text: PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    ")\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "# summary_ids = model.generate(inputs[\"input_ids\"].to(device), do_sample=True, num_beams=1, max_length=30)\n",
    "summary_ids = model.generate(inputs[\"input_ids\"].to(device), num_beams=2, min_length=0, max_length=30)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "\n",
    "# from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "# model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
    "# input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
    "# logits = model(input_ids).logits\n",
    "\n",
    "# masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "# probs = logits[0, masked_index].softmax(dim=0)\n",
    "# values, predictions = probs.topk(5)\n",
    "\n",
    "# tokenizer.decode(predictions).split()\n",
    "# ['not', 'good', 'healthy', 'great', 'very']\n",
    "# Inside GenerationMixin:model_inputs= dict_keys(['input_ids', 'encoder_outputs', 'past_key_values', 'decoder_input_ids', 'attention_mask', 'decoder_attention_mask', 'head_mask', 'decoder_head_mask', 'cross_attn_head_mask', 'use_cache'])\n",
    "# dict_keys(['encoder_outputs', 'past_key_values', 'decoder_input_ids', 'attention_mask' 'use_cache'])\n",
    "# Passed parameters:\n",
    "    # encoder_outputs\n",
    "    # past_key_values\n",
    "    # decoder_input_ids\n",
    "    # attention_mask\n",
    "    # use_cache = True 意味着会存储之前计算的 past_key_values 会被存储下来。 也就意味着，使用的是window attention。每次只计算新的token的kv，然后保存下来。\n",
    "# '''\n",
    "# encoder_outputs: torch.Size([2, 56, 1024]) 一值不变， 56 是输入序列长度，但是输入只有一个，也就是batch size 应该是1，但是这里是2.不知道为什么。\n",
    "# past_key_values: 12                        一值不变\n",
    "# decoder_input_ids: torch.Size([2, 1])      一值不变\n",
    "# attention_mask: torch.Size([2, 56])        一值不变\n",
    "# Inside BART: input_ids= tensor([[2], [2]]) 会变，这里可以看到的是，batch中的两个的值可能是一样的，也可能不是一样的。\n",
    "# Inside BART: input_ids= tensor([[1768], [ 717]])\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=1024, out_features=50264, bias=False), 50265)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head, tokenizer.vocab_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_2_1_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
