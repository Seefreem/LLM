{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seelur/enter/envs/pytorch_2_1_0/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "torch.cuda.empty_cache() # 清空GPU缓存，清空CUDA，清空CUDA内存\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = (\n",
    "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    ")\n",
    "\n",
    "QUERY_DOCUMENT = (\n",
    "    \"I am hacking into BART. I use different documents for encoder and decoder, and try to see the influence. \"\n",
    "    \"What do you think will happen?\"\n",
    ")\n",
    "doc1_inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n",
    "doc2_inputs = tokenizer([QUERY_DOCUMENT], max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Inside GenerationMixin:model_inputs= dict_keys(['input_ids', 'encoder_outputs', 'past_key_values', 'decoder_input_ids', 'attention_mask', 'decoder_attention_mask', 'head_mask', 'decoder_head_mask', 'cross_attn_head_mask', 'use_cache'])\n",
    "# dict_keys(['encoder_outputs', 'past_key_values', 'decoder_input_ids', 'attention_mask' 'use_cache'])\n",
    "# Passed parameters:\n",
    "    # encoder_outputs\n",
    "    # past_key_values\n",
    "    # decoder_input_ids\n",
    "    # attention_mask\n",
    "    # use_cache = True 意味着会存储之前计算的 past_key_values 会被存储下来。 也就意味着，使用的是window attention。每次只计算新的token的kv，然后保存下来。\n",
    "# '''\n",
    "# encoder_outputs: torch.Size([2, 56, 1024]) 一值不变， 56 是输入序列长度，但是输入只有一个，也就是batch size 应该是1，但是这里是2.不知道为什么。\n",
    "# past_key_values: 12                        一值不变\n",
    "# decoder_input_ids: torch.Size([2, 1])      一值不变\n",
    "# attention_mask: torch.Size([2, 56])        一值不变\n",
    "# Inside BART: input_ids= tensor([[2], [2]]) 会变，这里可以看到的是，batch中的两个的值可能是一样的，也可能不是一样的。\n",
    "# Inside BART: input_ids= tensor([[1768], [ 717]])\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用LLM 做生成任务的一般步骤：  \n",
    "1. 获得用户输入，添加开始字符\n",
    "2. 创建对应的 attention mask\n",
    "3. 设置模型为生成模式，可能需要设置是否cache KV\n",
    "4. tokenize，得到input_ids\n",
    "5. 写一个循环：\n",
    "    1. 判断是否达到终止条件\n",
    "    2. 运行模型，得到模型对下一个token的概率预测\n",
    "    3. 通过概率和采样算法获取下一个token的id -- 这被称之为 generation strategy https://huggingface.co/docs/transformers/en/generation_strategies  \n",
    "    对应的算法解释： https://huggingface.co/blog/how-to-generate\n",
    "    4. 将下一个token添加到input_ids序列中\n",
    "6. 将最终输出的 input_ids 输入给Tokenizer 进行decode，得到字符串。\n",
    "\n",
    "\n",
    "Example by Transformers module:\n",
    "1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n",
    "2. Set generation parameters if not already defined\n",
    "3. Define model inputs\n",
    "4. Define other model kwargs\n",
    "5. Prepare `input_ids` which will be used for auto-regressive generation\n",
    "6. Prepare `max_length` depending on other stopping criteria.\n",
    "7. determine generation mode\n",
    "8. prepare distribution pre_processing samplers\n",
    "9. prepare stopping criteria\n",
    "10. go into different generation modes, e.g. Beam search\n",
    "11. prepare beam search scorer \n",
    "12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "13. run beam search (i.e. generation loop)\n",
    "    1. stopping checking\n",
    "    2. predict next token\n",
    "    3. choose next token\n",
    "    4. concatenate\n",
    "\n",
    "所以对于我的需求（自定义cross attention的KV），我其实可以给模型写一个set函数，直接设置。或者模型提供对应的参数，那么我就通过参数传进去就好。然后把模型的forward中的encoder的代码注释掉就好。  \n",
    "或者呢我给模型多定义一个选择的参数。选择性使用Encoder即可。  \n",
    "也就是说，我还是使用Transformers提供的 generate函数，只用修改模型代码就好。  \n",
    "而且generate其实只对生成有影响，对Encoder其实没有影响。所以我要调用Encoder的话，我直接调用model.encoder(params)就行。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ calling Encoder\n",
      "Inside GenerationMinin:generation_mode=: GenerationMode.BEAM_SEARCH\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires\n"
     ]
    }
   ],
   "source": [
    "# Reference experiment: one document for Encoder, none document for Decoder\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(doc1_inputs[\"input_ids\"].to(device), num_beams=2, min_length=0, max_length=30)\n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ calling Encoder\n",
      "Inside GenerationMinin:generation_mode=: GenerationMode.BEAM_SEARCH\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: one document for Encoder, none document for Decoder\n",
    "\n",
    "# Encoder\n",
    "encoder_representations = model.get_encoder().forward(doc1_inputs[\"input_ids\"].to(device), doc1_inputs[\"attention_mask\"].to(device))\n",
    "# Decoder\n",
    "# 但至少可以先验证，如果提供了 encoder_outputs， 模型是否会正常输出: -- 可以正常输出\n",
    "# 这里的逻辑是，提供了 encoder_outputs 之后，在模型的forward函数中会跳过encoder\n",
    "summary_ids = model.generate(num_beams=2, min_length=0, max_length=30, \n",
    "                             encoder_outputs=encoder_representations) \n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ calling Encoder\n",
      "torch.Size([1, 56])\n",
      "Inside GenerationMinin:generation_mode=: GenerationMode.BEAM_SEARCH\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: one document for Encoder, none document for Decoder\n",
    "\n",
    "# Encoder\n",
    "encoder_representations = model.get_encoder().forward(doc1_inputs[\"input_ids\"].to(device), doc1_inputs[\"attention_mask\"].to(device))\n",
    "# Decoder\n",
    "# 验证添加 decoder_input_ids 会有什么影响 \n",
    "decoder_input_ids = shift_tokens_right(\n",
    "    doc1_inputs[\"input_ids\"].to(device), model.config.pad_token_id, model.config.decoder_start_token_id\n",
    ")\n",
    "print(doc1_inputs[\"input_ids\"].shape)\n",
    "summary_ids = model.generate(decoder_input_ids=decoder_input_ids, \n",
    "                             num_beams=2, min_length=0, max_length=doc1_inputs[\"input_ids\"].shape[1] + 30, \n",
    "                             encoder_outputs=encoder_representations) \n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "\n",
    "# document 1:\n",
    "# \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "# \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "# \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    "\n",
    "# 1. generate():\n",
    "# PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions. \n",
    "# The aim is to reduce the risk of wildfires\n",
    "\n",
    "# 2. decoder-only with decoder_input_ids and encoder_outputs from the same document:\n",
    "# PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. \n",
    "# The aim is to reduce the risk of wildfires. \n",
    "# Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to \n",
    "# last through at least midday tomorrow.\n",
    "\n",
    "# 结论：就是原封不动地输出了 decoder_input_ids 的内容。但是感觉也合理，首先，因为输入给Decoder的ids会全部输出，\n",
    "# 其次，因为 encoder_outputs 中没有额外的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ calling Encoder\n",
      "torch.Size([1, 33])\n",
      "Inside GenerationMinin:generation_mode=: GenerationMode.BEAM_SEARCH\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 56, 1024])\n",
      "Bart:  tensor([ 0.0138,  0.0368,  0.0209,  ...,  0.0042, -0.0008, -0.0026],\n",
      "       device='cuda:0')\n",
      "I am hacking into BART. I use different documents for encoder and decoder, and try to see the influence. What do you think will happen? Tell us in the comments below.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: one document for Encoder, another document for Decoder\n",
    "\n",
    "# Encoder\n",
    "encoder_representations = model.get_encoder().forward(doc1_inputs[\"input_ids\"].to(device), doc1_inputs[\"attention_mask\"].to(device))\n",
    "# Decoder\n",
    "decoder_input_ids = shift_tokens_right(\n",
    "    doc2_inputs[\"input_ids\"].to(device), model.config.pad_token_id, model.config.decoder_start_token_id\n",
    ")\n",
    "print(doc2_inputs[\"input_ids\"].shape)\n",
    "summary_ids = model.generate(decoder_input_ids=decoder_input_ids, \n",
    "                             num_beams=2, min_length=0, max_length=doc2_inputs[\"input_ids\"].shape[1] + 30, \n",
    "                             encoder_outputs=encoder_representations) \n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "\n",
    "# document 1:\n",
    "# \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "# \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "# \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    "\n",
    "# document 2:\n",
    "# \"I am hacking into BART. I use different documents for encoder and decoder, and try to see the influence. \"\n",
    "# \"What do you think will happen?\"\n",
    "\n",
    "# 1. generate():\n",
    "# PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions. \n",
    "# The aim is to reduce the risk of wildfires\n",
    "\n",
    "# 2. decoder-only with decoder_input_ids and encoder_outputs from the same document:\n",
    "# I am hacking into BART. I use different documents for encoder and decoder, and try to see the influence. \n",
    "# What do you think will happen? Tell us in the comments below.\n",
    "\n",
    "# 结论：就是原封不动地输出了 decoder_input_ids 的内容。然后还附加了一句 \"Tell us in the comments below.\"。 说明实验很成功。\n",
    "# 因为实际上输入给 Encoder的 文本 和 输入给Decoder的问题其实没有任何关系，因此不能将Encoder的输入内容添加到 Decoder的输出中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ calling Encoder\n",
      "torch.Size([1, 38, 1024])\n",
      "tensor([ 0.0190,  0.0207,  0.0212,  ..., -0.0005, -0.0067, -0.0023],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 33])\n",
      "Inside GenerationMinin:generation_mode=: GenerationMode.BEAM_SEARCH\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 38, 1024])\n",
      "Bart:  tensor([ 0.0190,  0.0207,  0.0212,  ..., -0.0005, -0.0067, -0.0023],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 38, 1024])\n",
      "Bart:  tensor([ 0.0190,  0.0207,  0.0212,  ..., -0.0005, -0.0067, -0.0023],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 38, 1024])\n",
      "Bart:  tensor([ 0.0190,  0.0207,  0.0212,  ..., -0.0005, -0.0067, -0.0023],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 38, 1024])\n",
      "Bart:  tensor([ 0.0190,  0.0207,  0.0212,  ..., -0.0005, -0.0067, -0.0023],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 38, 1024])\n",
      "Bart:  tensor([ 0.0190,  0.0207,  0.0212,  ..., -0.0005, -0.0067, -0.0023],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 38, 1024])\n",
      "Bart:  tensor([ 0.0190,  0.0207,  0.0212,  ..., -0.0005, -0.0067, -0.0023],\n",
      "       device='cuda:0')\n",
      "I am hacking into BART. I use different documents for encoder and decoder, and try to see the influence. What do you think will happen? I do not know!\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3: one document for Encoder, another document for Decoder. The two documents are related.\n",
    "\n",
    "# REFERENCF_DOCUMENT = (\n",
    "#     \"If you use different documents for encoder and decoder, \"\n",
    "#     \"the decoder will try to extract practical information from the reference of encoder.\"\n",
    "#     \" That means the words from the document for encoder may appear in the outputs of decoder.\"\n",
    "# )\n",
    "REFERENCF_DOCUMENT = QUERY_DOCUMENT + 'I do not know!'\n",
    "doc3_inputs = tokenizer([REFERENCF_DOCUMENT], max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Encoder\n",
    "encoder_representations = model.get_encoder().forward(doc3_inputs[\"input_ids\"].to(device), doc3_inputs[\"attention_mask\"].to(device))\n",
    "print(encoder_representations[0].shape) # torch.Size([1, 49, 1024])\n",
    "print(encoder_representations[0][0, 0, :])\n",
    "# Decoder\n",
    "decoder_input_ids = shift_tokens_right(\n",
    "    doc2_inputs[\"input_ids\"].to(device), model.config.pad_token_id, model.config.decoder_start_token_id\n",
    ")\n",
    "print(doc2_inputs[\"input_ids\"].shape)\n",
    "summary_ids = model.generate(decoder_input_ids=decoder_input_ids, \n",
    "                             num_beams=2, min_length=0, max_length=doc2_inputs[\"input_ids\"].shape[1] + 30, \n",
    "                             encoder_outputs=encoder_representations) \n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "\n",
    "# 问题： \n",
    "# 1. 不知道模型在哪里调用了 Encoder，\n",
    "\n",
    "# document 1:\n",
    "# \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "# \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "# \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    "\n",
    "# document 2:\n",
    "# \"I am hacking into BART. I use different documents for encoder and decoder, and try to see the influence. \"\n",
    "# \"What do you think will happen?\"\n",
    "\n",
    "# document 3:\n",
    "# \"If you use different documents for encoder and decoder, \"\n",
    "# \"the decoder will try to extract practical information from the reference of encoder.\"\n",
    "# \" That means the words from the document for encoder may appear in the outputs of decoder.\"\n",
    "\n",
    "# 1. generate():\n",
    "# PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions. \n",
    "# The aim is to reduce the risk of wildfires\n",
    "\n",
    "# 2. decoder-only with decoder_input_ids and encoder_outputs from the same document:\n",
    "# I am hacking into BART. I use different documents for encoder and decoder, and try to see the influence. \n",
    "# What do you think will happen? \n",
    "# I do not know!\n",
    "\n",
    "# 结论：就是原封不动地输出了 decoder_input_ids 的内容。然后还附加了一句 \"I do not know!\"。新增的这句话是来自于 reference document的。\n",
    "# 说明实验很成功。\n",
    "# 因为实际上输入给 Encoder的 文本 和 输入给Decoder的问题其实没有任何关系，因此不能将Encoder的输入内容添加到 Decoder的输出中。\n",
    "# 存在的缺陷，模型只能续写，而不能进行推理。因为当我把document3 换成了相关但是需要推理的文本时，模型就直接忽略了 document3.\n",
    "# Decoder 的行为主要还是复述 encoder中的内容，但是是基于自己的input来进行复述的。当两者的输入没有直接关系时，Decoder就会忽视encoder的内容。\n",
    "# 一个猜测是，这种情况下的Decoder其实是在做一个转述/翻译的工作，而没有推理能力，或者说，没有in-context-learning的能力。所以会忽略掉 Decoder\n",
    "# 给它的参考内容。\n",
    "# 那么验证这个猜想的办法就是微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ calling Encoder\n",
      "torch.Size([1, 58, 1024])\n",
      "tensor([ 0.0191,  0.0232,  0.0188,  ...,  0.0005, -0.0022, -0.0001],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 33])\n",
      "Inside GenerationMinin:generation_mode=: GenerationMode.BEAM_SEARCH\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 58, 1024])\n",
      "Bart:  tensor([ 0.0191,  0.0232,  0.0188,  ...,  0.0005, -0.0022, -0.0001],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 58, 1024])\n",
      "Bart:  tensor([ 0.0191,  0.0232,  0.0188,  ...,  0.0005, -0.0022, -0.0001],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 58, 1024])\n",
      "Bart:  tensor([ 0.0191,  0.0232,  0.0188,  ...,  0.0005, -0.0022, -0.0001],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 58, 1024])\n",
      "Bart:  tensor([ 0.0191,  0.0232,  0.0188,  ...,  0.0005, -0.0022, -0.0001],\n",
      "       device='cuda:0')\n",
      "Calling forward() False\n",
      "Bart:  torch.Size([2, 58, 1024])\n",
      "Bart:  tensor([ 0.0191,  0.0232,  0.0188,  ...,  0.0005, -0.0022, -0.0001],\n",
      "       device='cuda:0')\n",
      "I am hacking into BART. I use different documents for encoder and decoder, and try to see the influence. What do you think will happen? Share your thoughts.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 4: one document for Encoder, another document for Decoder. The two documents are related.\n",
    "\n",
    "REFERENCF_DOCUMENT =  (\n",
    "    \"What do you think will happen?\"\n",
    "    \"If you use different documents for encoder and decoder in BART, \"\n",
    "    \"the decoder will try to extract practical information from the reference of encoder.\"\n",
    "    \" That means the words from the document for encoder may appear in the outputs of decoder.\"\n",
    ")\n",
    "doc3_inputs = tokenizer([REFERENCF_DOCUMENT], max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Encoder\n",
    "encoder_representations = model.get_encoder().forward(doc3_inputs[\"input_ids\"].to(device), doc3_inputs[\"attention_mask\"].to(device))\n",
    "print(encoder_representations[0].shape) # torch.Size([1, 49, 1024])\n",
    "print(encoder_representations[0][0, 0, :])\n",
    "# Decoder\n",
    "decoder_input_ids = shift_tokens_right(\n",
    "    doc2_inputs[\"input_ids\"].to(device), model.config.pad_token_id, model.config.decoder_start_token_id\n",
    ")\n",
    "print(doc2_inputs[\"input_ids\"].shape)\n",
    "summary_ids = model.generate(decoder_input_ids=decoder_input_ids, \n",
    "                             num_beams=2, min_length=0, max_length=doc2_inputs[\"input_ids\"].shape[1] + 30, \n",
    "                             encoder_outputs=encoder_representations) \n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "\n",
    "# 问题： \n",
    "# 1. 不知道模型在哪里调用了 Encoder，\n",
    "\n",
    "# document 1:\n",
    "# \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "# \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "# \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    "\n",
    "# document 2:\n",
    "# \"I am hacking into BART. I use different documents for encoder and decoder, and try to see the influence. \"\n",
    "# \"What do you think will happen?\"\n",
    "\n",
    "# document 3:\n",
    "# \"What do you think will happen?\"\n",
    "# \"If you use different documents for encoder and decoder in BART, \"\n",
    "# \"the decoder will try to extract practical information from the reference of encoder.\"\n",
    "# \" That means the words from the document for encoder may appear in the outputs of decoder.\"\n",
    "\n",
    "# 1. generate():\n",
    "# PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions. \n",
    "# The aim is to reduce the risk of wildfires\n",
    "\n",
    "# 2. decoder-only with decoder_input_ids and encoder_outputs from the same document:\n",
    "# I am hacking into BART. I use different documents for encoder and decoder, and try to see the influence. \n",
    "# What do you think will happen? Share your thoughts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "fine-tuning \n",
    "https://github.com/facebookresearch/fairseq/tree/main/examples/bart\n",
    "https://github.com/facebookresearch/fairseq/blob/main/examples/bart/README.glue.md\n",
    "https://gluebenchmark.com/tasks\n",
    "https://github.com/nyu-mll/GLUE-baselines\n",
    "https://huggingface.co/facebook/bart-large\n",
    "https://huggingface.co/docs/transformers/training#train-in-native-pytorch\n",
    "https://github.com/Mooler0410/LLMsPracticalGuide?tab=readme-ov-file\n",
    "https://medium.com/@ferlatti.aldo/fine-tuning-a-chat-summarizer-c18625bc817d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可用的数据集：  \n",
    "DROP, LAMBADA, CBT (Children’s Book Test), RACE, SQuAD (Stanford Question Answering Dataset), SuperGLUE-boolq, SuperGLUE-copa, GSM8k(或许也可以用？), AlpacaEval(历史数据可以视为参考数据), AlpacaEval(instruction follow), MT Bench (Multi-Turn Benchmark), MMMU(或许部分数据集可用), MMLU, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Generative loop example (from https://www.junronglin.com/article/why_left_padding):\n",
    "def sample_sequence(model, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, device='cuda', sample=True, enc=None):\n",
    "    if start_token is None:\n",
    "\t\t\t\t # if start_token is None, use context\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "        context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(\n",
    "            0).repeat(batch_size, 1)\n",
    "    else:\n",
    "\t\t\t\t # if start_token isn't None, use start_token as the beginning of each sentences\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = torch.full((batch_size, 1), start_token,\n",
    "                             device=device, dtype=torch.long)\n",
    "    prev = context\n",
    "    output = context\n",
    "    # past is KV-cache\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "        for i in trange(length):  # generate `length` tokens for all sentences\n",
    "            logits, past = model(prev, past=past)\n",
    "\n",
    "            # logits.shape=[batch, text, vocab_szie], in Causal model, the logits of the last token in each sentence is used to predict next token, so pick `-1` here\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                prev = torch.multinomial(log_probs, num_samples=1)\n",
    "            else:\n",
    "                _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "\n",
    "            # concatenate the sampled tokens to the original sentences, \n",
    "            # e.g. output = [I have] and sampled `an`\n",
    "            # output = [I have an]\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_2_1_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
