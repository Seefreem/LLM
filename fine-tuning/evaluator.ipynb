{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 快速入门 A quick tour\n",
    "https://huggingface.co/docs/evaluate/en/a_quick_tour  \n",
    "总结：  \n",
    "1. evaluation又两种方法，第一是拿到模型的输出，直接和ground truth 进行比较；另一种是使用集成好的 evaluator\n",
    "2. evaluator 接受三个主要参数：model、data、metric。但是也是可以指定数据中的那个column是模型的输入，哪个column是ground truth\n",
    "3. 所有评估方法被分为三类：metric、comparison和measurement，他们分别有自己的‘space’，在space中可以看到有哪些评估方法、定义和案例\n",
    "4. 可以通过 evaluation 的 features 属性查看输入的基本数据类型\n",
    "5. 支持一次性、累计和分布式的评估\n",
    "6. 可以结合多个指标进行评估\n",
    "7. 可以将评估结果可视化\n",
    "8. 可以将评估结果保存为JSON文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two common types of question answering:\n",
    "\n",
    "- extractive: given a question and some context, the answer is a span of text from the context the model must extract  \n",
    "- abstractive: given a question and some context, the answer is generated from the context; this approach is handled by the Text2TextGenerationPipeline instead of the QuestionAnsweringPipeline shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9327, start: 30, end: 54, answer: huggingface/transformers\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Extractive QA:\n",
    "question_answerer = pipeline(task=\"question-answering\")\n",
    "preds = question_answerer(\n",
    "    question=\"What is the name of the repository?\",\n",
    "    context=\"The name of the repository is huggingface/transformers\",\n",
    ")\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# Abstractive QA:\n",
    "generator = pipeline(model=\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "generator(\n",
    "    \"The name of the repository is huggingface/transformers. What is the name of the repository?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/evaluate/en/base_evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluator\n",
    "from datasets import load_dataset\n",
    "task_evaluator = evaluator(\"text2text-generation\")\n",
    "data = load_dataset(\"conll2003\", split=\"validation[:2]\")\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=\"elastic/distilbert-base-uncased-finetuned-conll03-english\",\n",
    "    data=data,\n",
    "    metric=\"seqeval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions1': Value(dtype='int64', id=None), 'predictions2': Value(dtype='int64', id=None)}\n",
      "\n",
      "Returns the rate at which the predictions of one model exactly match those of another model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import evaluate \n",
    "exact_match = evaluate.load(\"exact_match\", module_type=\"comparison\")\n",
    "print(exact_match.features) # 查看输入数据类型\n",
    "# results = exact_match.compute(predictions1=\"test1\", predictions2=\"test\") # 难怪不能检测字符串\n",
    "# print(results)\n",
    "print(exact_match.description) # 查看相关信息\n",
    "# 其他相关信息：\n",
    "# https://huggingface.co/docs/evaluate/en/a_quick_tour Module attributes\n",
    "# Attribute\tDescription\n",
    "# description\tA short description of the evaluation module.\n",
    "# citation\tA BibTex string for citation when available.\n",
    "# features\tA Features object defining the input format.\n",
    "# inputs_description\tThis is equivalent to the modules docstring.\n",
    "# homepage\tThe homepage of the module.\n",
    "# license\tThe license of the module.\n",
    "# codebase_urls\tLink to the code behind the module.\n",
    "# reference_urls\tAdditional reference URLs.\n",
    "\n",
    "# features中说的是基础数据单元的类型，实际上完全是可以接受list等集合的。\n",
    "# Note that features always describe the type of a single input element. \n",
    "# In general we will add lists of elements so you can always think of a list around the types in features. \n",
    "# Evaluate accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc.) and \n",
    "# converts them to an appropriate format for storage and computation.\n",
    "\n",
    "'''\n",
    "至于 evaluator 是怎么知道什么数据是输入，什么是输出的，在evaluator的参数表中有默认值：\n",
    "    input_column: str = \"text\",\n",
    "    label_column: str = \"label\",\n",
    "\n",
    "所以其实可以自己指定，但是不能指定多个input column，所以针对你的情况，你还是不能直接使用 evaluator。因为你的输入不止一个。\n",
    "\n",
    "还可以指定 tokenizer 等\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何选择合适的评估方法 Choosing a metric for your task\n",
    "\n",
    "一般有三种确定评估函数的方法：\n",
    "1. 选择一般的评估方法 Generic metrics\n",
    "2. 根据任务选择评估方法 Task-specific metrics\n",
    "3. 根据数据集选择评估方法，因为有些数据集自己带了评估方法（dedicated evaluation metric） Dataset-specific metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic metrics: accuracy and precision  \n",
    "accuracy: 其实就是计算预测正确的正负样本的百分比。只支持数值类型的标签\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-specific metrics:    \n",
    "这有很多中评估方式，可以在对应的task页面查看‘metrics’，得到评估方式。https://huggingface.co/tasks   \n",
    "\n",
    "https://medium.com/@sharathhebbar24/text-generation-v-s-text2text-generation-3a2b235ac19b  \n",
    "Text Generation, also known as Causal Language Modeling, is the process of generating text that closely resembles human writing.  \n",
    "Text-to-Text Generation, also known as Sequence-to-Sequence Modeling, is the process of converting one piece of text into another. See examples from T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
