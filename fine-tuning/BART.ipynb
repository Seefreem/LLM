{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seelur/enter/envs/pytorch_2_1_0/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/seelur/enter/envs/pytorch_2_1_0/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large') # param 400M, file size 1G \n",
    "model = BartModel.from_pretrained('facebook/bart-large')\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# last_hidden_states = outputs.last_hidden_state\n",
    "# print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 8])\n",
      "input_ids attention_mask\n"
     ]
    }
   ],
   "source": [
    "print(type(inputs))\n",
    "print(inputs.keys())\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(inputs[\"attention_mask\"].shape)\n",
    "print(*inputs) # keays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device) # 使用GPU\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside BART: Encoder: input_ids= tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]],\n",
      "       device='cuda:0')\n",
      "Inside BART: input_ids= tensor([[    2,     0, 31414,     6,   127,  2335,    16, 11962]],\n",
      "       device='cuda:0')\n",
      "Inside BART: attention_mask= None\n",
      "Inside BART: encoder_hidden_states= tensor([[[-0.0089,  0.0095,  0.0101,  ...,  0.0054, -0.0061, -0.0019],\n",
      "         [-0.0242, -0.2558,  0.1059,  ..., -0.1760,  0.1102,  0.0296],\n",
      "         [ 0.0278, -0.2268,  0.0643,  ...,  0.0303,  0.1538, -0.1770],\n",
      "         ...,\n",
      "         [-0.0684, -0.1958, -0.0589,  ..., -0.0166,  0.1663, -0.1595],\n",
      "         [-0.0161, -0.3884, -0.4422,  ..., -0.0780,  0.0474, -0.0166],\n",
      "         [ 0.0912,  0.0469, -0.0327,  ..., -0.0290, -0.0870,  0.2132]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "Inside BART: encoder_attention_mask= tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Inside BART: head_mask= None\n",
      "Inside BART: cross_attn_head_mask= None\n",
      "Inside BART: past_key_values= None\n",
      "Inside BART: inputs_embeds= None\n",
      "Inside BART: use_cache= True\n",
      "Inside BART: output_attentions= False\n",
      "Inside BART: output_hidden_states= False\n",
      "Inside BART: return_dict= True\n",
      "tensor([[[ 0.5512,  0.8389, -1.4707,  ...,  1.3124, -0.2047,  0.2392],\n",
      "         [ 0.5512,  0.8389, -1.4707,  ...,  1.3124, -0.2047,  0.2392],\n",
      "         [ 0.9143,  0.9399, -1.2426,  ...,  0.9184, -0.1838, -0.9975],\n",
      "         ...,\n",
      "         [ 0.2561,  0.2253,  0.4470,  ...,  0.3447,  0.0087,  1.5508],\n",
      "         [ 0.2077, -1.3086, -1.4295,  ..., -0.2998,  0.1828,  0.4700],\n",
      "         [-0.4893,  2.5148, -1.5513,  ...,  0.5783,  1.0961,  0.1736]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = model(input_ids, attention_mask) # 所以模型的输入是 input_ids 和 attention_mask\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs['last_hidden_state'].shape)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualization # 模型是ONNX格式的，不能直接可视化\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter('./BART')\n",
    "\n",
    "# # add_graph() will trace the sample input through your model,\n",
    "# # and render it as a graph.\n",
    "# # writer.add_graph(model, input_ids, attention_mask)\n",
    "# writer.add_graph(model, inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "# writer.flush()\n",
    "\n",
    "# # To view, start TensorBoard on the command line with:\n",
    "# #   tensorboard --logdir=runs\n",
    "# # ...and open a browser tab to http://localhost:6006/\n",
    "# # 注意上面的'runs'是日志保存的路径，在前面的代码中有设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartModel(\n",
       "  (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "  (encoder): BartEncoder(\n",
       "    (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoder(\n",
       "  (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "  (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x BartDecoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (activation_fn): GELUActivation()\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 只使用embedder进行运算，得到encoder的运算结果 \n",
    "embeddings = model.shared(input_ids) # 所以模型的输入是 input_ids 和 attention_mask\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside BART: Encoder: input_ids= tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]],\n",
      "       device='cuda:0')\n",
      "odict_keys(['last_hidden_state'])\n",
      "torch.Size([1, 8, 1024])\n",
      "tensor([[[-0.0089,  0.0095,  0.0101,  ...,  0.0054, -0.0061, -0.0019],\n",
      "         [-0.0242, -0.2558,  0.1059,  ..., -0.1760,  0.1102,  0.0296],\n",
      "         [ 0.0278, -0.2268,  0.0643,  ...,  0.0303,  0.1538, -0.1770],\n",
      "         ...,\n",
      "         [-0.0684, -0.1958, -0.0589,  ..., -0.0166,  0.1663, -0.1595],\n",
      "         [-0.0161, -0.3884, -0.4422,  ..., -0.0780,  0.0474, -0.0166],\n",
      "         [ 0.0912,  0.0469, -0.0327,  ..., -0.0290, -0.0870,  0.2132]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 只使用encoder进行运算，得到encoder的运算结果，注意调用encoder的时候它默认调佣 embedder\n",
    "encoder_outputs = model.encoder(input_ids, attention_mask) # 所以模型的输入是 input_ids 和 attention_mask\n",
    "representations = encoder_outputs.last_hidden_state\n",
    "print(encoder_outputs.keys())\n",
    "print(representations.shape)\n",
    "print(representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside BART: input_ids= tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]],\n",
      "       device='cuda:0')\n",
      "Inside BART: attention_mask= tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Inside BART: encoder_hidden_states= None\n",
      "Inside BART: encoder_attention_mask= None\n",
      "Inside BART: head_mask= None\n",
      "Inside BART: cross_attn_head_mask= None\n",
      "Inside BART: past_key_values= None\n",
      "Inside BART: inputs_embeds= None\n",
      "Inside BART: use_cache= None\n",
      "Inside BART: output_attentions= None\n",
      "Inside BART: output_hidden_states= None\n",
      "Inside BART: return_dict= None\n",
      "odict_keys(['last_hidden_state', 'past_key_values'])\n",
      "torch.Size([1, 8, 1024])\n",
      "<class 'tuple'> 12\n",
      "<class 'tuple'> 2\n",
      "<class 'torch.Tensor'> 4\n",
      "tensor([[[-0.7063,  1.2007,  0.1487,  ...,  0.4761, -0.7794, -1.1263],\n",
      "         [-0.7752,  1.0035,  0.8857,  ...,  1.0058,  0.2381, -1.0515],\n",
      "         [-0.2852, -1.1637,  1.8671,  ..., -0.6132,  0.3621, -1.3947],\n",
      "         ...,\n",
      "         [-1.4383,  0.5772,  2.3473,  ...,  0.7001,  0.2143,  1.4135],\n",
      "         [-0.4589, -0.1476,  2.2830,  ...,  0.1734, -0.4468,  0.6591],\n",
      "         [-0.5654,  0.1650,  2.2447,  ...,  0.9836,  0.2274,  0.6033]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 只使用decoder进行运算，得到decoder的运算结果，注意调用decoder的时候它默认调佣 embedder\n",
    "intermediate_output = model.decoder(input_ids, attention_mask) # 所以模型的输入是 input_ids 和 attention_mask\n",
    "predictions = intermediate_output.last_hidden_state\n",
    "print(intermediate_output.keys())\n",
    "print(predictions.shape)\n",
    "print(type(intermediate_output['past_key_values']), len(intermediate_output['past_key_values'])) # 这个应该是指这次保存的 key 和 value值，可以用来做 window attention\n",
    "print(type(intermediate_output['past_key_values'][0]), len(intermediate_output['past_key_values'][0]))\n",
    "print(type(intermediate_output['past_key_values'][0][0]), len(intermediate_output['past_key_values'][0][0].shape))\n",
    "print(predictions) # 这里输出的结果和运行完整模型的结果不一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在的问题是，怎么将 encoder 的输出 单独输入给 decoder？\n",
    "# 是否可以直接给模型的参数赋值？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.ModuleList'> 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartDecoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (activation_fn): GELUActivation()\n",
       "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# representations torch.Size([1, 8, 1024])\n",
    "print(type(model.decoder.layers), len(model.decoder.layers)) # <class 'torch.nn.modules.container.ModuleList'>  12\n",
    "model.decoder.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside BART: input_ids= tensor([[    2,     0, 31414,     6,   127,  2335,    16, 11962]],\n",
      "       device='cuda:0')\n",
      "Inside BART: attention_mask= None\n",
      "Inside BART: encoder_hidden_states= tensor([[[-0.0089,  0.0095,  0.0101,  ...,  0.0054, -0.0061, -0.0019],\n",
      "         [-0.0242, -0.2558,  0.1059,  ..., -0.1760,  0.1102,  0.0296],\n",
      "         [ 0.0278, -0.2268,  0.0643,  ...,  0.0303,  0.1538, -0.1770],\n",
      "         ...,\n",
      "         [-0.0684, -0.1958, -0.0589,  ..., -0.0166,  0.1663, -0.1595],\n",
      "         [-0.0161, -0.3884, -0.4422,  ..., -0.0780,  0.0474, -0.0166],\n",
      "         [ 0.0912,  0.0469, -0.0327,  ..., -0.0290, -0.0870,  0.2132]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "Inside BART: encoder_attention_mask= tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Inside BART: head_mask= None\n",
      "Inside BART: cross_attn_head_mask= None\n",
      "Inside BART: past_key_values= None\n",
      "Inside BART: inputs_embeds= None\n",
      "Inside BART: use_cache= True\n",
      "Inside BART: output_attentions= None\n",
      "Inside BART: output_hidden_states= None\n",
      "Inside BART: return_dict= True\n",
      "odict_keys(['last_hidden_state', 'past_key_values'])\n",
      "torch.Size([1, 8, 1024])\n",
      "<class 'tuple'> 12\n",
      "<class 'tuple'> 4\n",
      "<class 'torch.Tensor'> 4\n",
      "tensor([[[ 0.5512,  0.8389, -1.4707,  ...,  1.3124, -0.2047,  0.2392],\n",
      "         [ 0.5512,  0.8389, -1.4707,  ...,  1.3124, -0.2047,  0.2392],\n",
      "         [ 0.9143,  0.9399, -1.2426,  ...,  0.9184, -0.1838, -0.9975],\n",
      "         ...,\n",
      "         [ 0.2561,  0.2253,  0.4470,  ...,  0.3447,  0.0087,  1.5508],\n",
      "         [ 0.2077, -1.3086, -1.4295,  ..., -0.2998,  0.1828,  0.4700],\n",
      "         [-0.4893,  2.5148, -1.5513,  ...,  0.5783,  1.0961,  0.1736]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOK，通过打印在两种使用情况下 Decoder接收到的输入，发现，一个关键点，那就是 在EncoderDecoder架构中，\\nDecoder接收到的input_ids是被shuffle过的。为什么？\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只使用decoder进行运算，得到decoder的运算结果，注意调用decoder的时候它默认调佣 embedder\n",
    "temp_ids = torch.tensor([[    2,     0, 31414,     6,   127,  2335,    16, 11962]]).to(device)\n",
    "intermediate_output = model.decoder(temp_ids, \n",
    "                                    encoder_hidden_states=encoder_outputs[0], \n",
    "                                    encoder_attention_mask=attention_mask,\n",
    "                                    use_cache=True,\n",
    "                                    return_dict=True) # 所以模型的输入是 input_ids 和 attention_mask\n",
    "predictions = intermediate_output.last_hidden_state\n",
    "print(intermediate_output.keys())\n",
    "print(predictions.shape)\n",
    "print(type(intermediate_output['past_key_values']), len(intermediate_output['past_key_values'])) # 这个应该是指这次保存的 key 和 value值，可以用来做 window attention\n",
    "print(type(intermediate_output['past_key_values'][0]), len(intermediate_output['past_key_values'][0]))\n",
    "print(type(intermediate_output['past_key_values'][0][0]), len(intermediate_output['past_key_values'][0][0].shape))\n",
    "print(predictions) # 这里输出的结果和运行完整模型的结果不一致\n",
    "\n",
    "'''\n",
    "OK，通过打印在两种使用情况下 Decoder接收到的输入，发现，一个关键点，那就是 在EncoderDecoder架构中，\n",
    "Decoder接收到的input_ids是被shuffle过的。为什么？发生在什么时候？\n",
    "破案了，当直接使用EncoderDecoder的时候，如果没有传入 decoder_input_ids， 那么就会通过对 input_ids 调用 shift_tokens_right 函数\n",
    "得到decoder_input_ids。该函数实现向右移动一个token的功能。\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/home/seelur/enter/lib/python3.11/site-packages/urllib3/util/ssl_.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_metrics\n\u001b[1;32m      2\u001b[0m ml \u001b[38;5;241m=\u001b[39m list_metrics()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(ml)\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/datasets/__init__.py:43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pyarrow\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/datasets/arrow_dataset.py:66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xgetsize\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/datasets/arrow_writer.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Features, Image, Value\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     FeatureType,\n\u001b[1;32m     30\u001b[0m     _ArrayXDExtensionType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     to_pyarrow_listarray,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_remote_filesystem\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/datasets/features/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray2D\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslationVariableLanguages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m ]\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, Sequence, Value\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/datasets/features/audio.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xopen, xsplitext\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_cast\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m no_op_if_value_is_null, string_to_dict\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/datasets/download/__init__.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadManager, DownloadMode\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingDownloadManager\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maiohttp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientError\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPRESSION_FILESYSTEMS\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     get_authentication_headers_for_url,\n\u001b[1;32m     24\u001b[0m     http_head,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     url_or_path_join,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/datasets/filesystems/__init__.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m _has_s3fs \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3fs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_s3fs:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ms3filesystem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3FileSystem  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     18\u001b[0m COMPRESSION_FILESYSTEMS: List[compression\u001b[38;5;241m.\u001b[39mBaseCompressedFileFileSystem] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     compression\u001b[38;5;241m.\u001b[39mBz2FileSystem,\n\u001b[1;32m     20\u001b[0m     compression\u001b[38;5;241m.\u001b[39mGzipFileSystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     compression\u001b[38;5;241m.\u001b[39mZstdFileSystem,\n\u001b[1;32m     24\u001b[0m ]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Register custom filesystems\u001b[39;00m\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/datasets/filesystems/s3filesystem.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01ms3fs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mS3FileSystem\u001b[39;00m(s3fs\u001b[38;5;241m.\u001b[39mS3FileSystem):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    `datasets.filesystems.S3FileSystem` is a subclass of [`s3fs.S3FileSystem`](https://s3fs.readthedocs.io/en/latest/api.html).\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/s3fs/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3FileSystem, S3File\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3Map\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/s3fs/core.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maiobotocore\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maiobotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maiobotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AioConfig\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientError, HTTPClientError, ParamValidationError\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/aiobotocore/session.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNSIGNED, translate\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PartialCredentialsError\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     EVENT_ALIASES,\n\u001b[1;32m      5\u001b[0m     ServiceModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     copy,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/botocore/translate.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2012-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge_dicts\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_retry_config\u001b[39m(\n\u001b[1;32m     20\u001b[0m     endpoint_prefix, retry_model, definitions, client_retry_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     21\u001b[0m ):\n\u001b[1;32m     22\u001b[0m     service_config \u001b[38;5;241m=\u001b[39m retry_model\u001b[38;5;241m.\u001b[39mget(endpoint_prefix, {})\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/botocore/utils.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mawsrequest\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttpsession\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# IP Regexes retained for backwards compatibility\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HEX_PAT  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/botocore/httpsession.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m URLLib3SSLError\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Retry\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mssl_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     DEFAULT_CIPHERS,\n\u001b[1;32m     24\u001b[0m     OP_NO_COMPRESSION,\n\u001b[1;32m     25\u001b[0m     PROTOCOL_TLS,\n\u001b[1;32m     26\u001b[0m     OP_NO_SSLv2,\n\u001b[1;32m     27\u001b[0m     OP_NO_SSLv3,\n\u001b[1;32m     28\u001b[0m     is_ipaddress,\n\u001b[1;32m     29\u001b[0m     ssl,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01murl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_url\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/home/seelur/enter/lib/python3.11/site-packages/urllib3/util/ssl_.py)"
     ]
    }
   ],
   "source": [
    "from datasets import list_metrics\n",
    "ml = list_metrics()\n",
    "print(ml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
