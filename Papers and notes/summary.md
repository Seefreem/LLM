总的来说，BERT、GPT、Falcon、PaLM、LlaMA等大模型都是基于Transformer架构的。
它们的创新点基本是基于对Transformer的改进方法和对数据的改进。
这个阶段的创新度并不强，偏工程。

基本目标都是：做大(增加参数)，做强(模型优化)，做精(数据优化)，做多(增多任务)




大模型分支：
BERT分支：也就是 encoder-only 分支，只是用Transformer 的encoder。类似于一个分析器，能有效分析文本内容。
    关键模型：
GPT分支：也就是 decoder-only 分支，类似于一个生成器，能有效生成文本内容。
    关键分支：PlaM、LlaMA
T5分支：encoder-decoder分支，了解较少。
    关键模型：GLM、Chat GLM、Falcon





目前为止你学习了些什么？
LangChain -> LLM Applications
Transformer
BERT
GPT 1/2/3
PaLM 1/2
Llama 1/2
Falcon
LoRA
Fine-tuning

如果只是看到这些词，你能联想到哪些信息呢？
在这些学习中你有哪些感触？你觉得什么是可以改进的？你有哪些创意灵感？

你现在具有的新能力有哪些？
1. 能理解基于Transformer架构的神经网络，理解注意力机制是怎么运作的。
2. 能够微调LLM模型，从而提升其在LangChain上的性能。
3. 能够基于LangChain开发简单的应用。


改进：




灵感：






