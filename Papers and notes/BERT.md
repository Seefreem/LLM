首先，BERT在训练时的目标任务有两个：第一，给定一个句子，预测下一个句子（实际上是将两个句子都输入，模型判断两个句子是否是连在一起的）；第二，预测句子中缺失的单词（完形填空）。
据说BERT的目标是理解句子含义。也因此能够在不改变BERT参数，而仅仅增加输出全连接层的情况下就能获得较好的下游任务结果。
但是据说BERT这两个学习任务本身其实没有什么实际应用。
通常的做法是，将BERT的输出有选择性地输入到全连接层，以适配下游任务。比如将第一个向量输入给下游任务，进行句子层面的语义分析，比如情感分析。
比如将句子对应的输出向量输入给下游任务，从而进行翻译、问答、NLI等任务。

BERT学习了两方面的representations，第一词汇的representation，第二句子的representation。
学习tokens的representations本身就能学习到单词的含义，也就是能理解单词。
判断两个句子是否是连续的，可以从概率的角度来看。但是也可以从语义的角度来看。如果两个句子的语义近似，并且语义存在一个先后关系。那么两个句子就是连续的。如果是这样，那么BERT确实还是能学到句子层面的语义信息的。可以作为一个sentence embedding model。

另外需要注意一下BERT的bidirectional 并不是指有两个BERT，而是指在生成某一个单词的representation的时候，能够看到这个单词的前后文（上下文，context）。

不过说实话，BERT应该也是能用于作为生成模型的。给未写完的句子的末尾加上一个[MASK]标签，那么模型就能输出对这个词的预测，然后将这个representation映射为词，再输入给BERT。依次循环。
下游任务实际上是没有[mask]这个token的，因为下游任务不会是完型填空。但是你也可以这么干。因此原作者的设计中，[mask]其实有三种做法，第一是真的填一个MASK上去，第二是随机选择一个另外的文字token，第三是什么都不做修改。这样的操作，其实就是只有两种MASK，而且这样，其实能让模型更加鲁棒。因为模型能够在输入有误的情况下精准理解句子含义。也就具备了识别错误的能力。

# 一些别人的经验
1. BERT的不同层学到了不同的语义信息和结构信息。因此可以根据这些特征对任务的作用来微调模型。
2. 微调BERT可以是微调整个BERT模型，可以是微调其中的几个layer，可以是一个layer，还可以是只学习附加的全连接层。这个都可以试试，看最终效果。
3. 微调也可以根据数据来安排。首先是领域内微调，然后是多任务微调，最后是任务内微调。多领域的微调的效果似乎不好。
4. 微调的时候，可以给不同layer设置不同的learning rate。
5. 微调的三个工作方向：以模型为中心，以数据为中心，以任务为中心。然后基于此，他们给出了一个微调BERT的一般步骤，首先进行任务内和领域内微调，然后是多任务微调，最后是目标任务微调。但是这几个术语还是比较模糊。
参考 https://zhuanlan.zhihu.com/p/386603816 这篇文章来进行微调。

在哪儿寻找现成的代码？Hugging Face



# 问题
有四个问题：
-- 第一，为什么这两个任务能够让BERT理解自然语言？
-- 第二，怎么利用BERT进行微调？
第三，BERT的优势在哪里？
第四，BERT的局限性在哪里？



















