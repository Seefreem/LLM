目标将模型输出和用户align。
方法则是fine-tuning with human feedback。

首先是人工生成了一些带标签的数据。拿来训练模型。
训练之后，让模型生成一系列的结果，对同一个问题生成多个回答。将回答那给人打分，排名。将好的结果输入给模型进行强化学习。得到的结果就是InstructGPT。

Mu Li对ChatGPT的评价：目前还是玩具还不是工具。工具的一个特点是稳定可靠，工具不需要多惊人。但一定要稳定可靠。


据说这篇论文还详细讲了怎么标注数据，以及怎么在实际生产过程中找合同工。
据说大模型训练的难点之一就是训练过程不稳定，可能loss在训练过程中变得很大。
在LM中，有些场景下居然过拟合也是有帮助的。

据说这篇论文给想要将预训练大模型微调到自己的问题上的人提供了一个参考方法。


这篇论文中提出了三个步骤：
S1 人工生成带标注的数据。训练得到SFT（supervised fine-tuning model）。
S2 让模型针对每个prompt生成9个答案，然后让人来给答案排序。将这个排序结果用于训练一个RM（reward model）。
S3 将SFT和RM放在强化学习PPO框架下，对SFT模型进行强化学习。


但是回到模型的名字上来看，instructGPT，它真的能听懂人类的instruction了吗？



