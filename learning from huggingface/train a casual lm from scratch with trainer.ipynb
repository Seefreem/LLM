{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified version of Huggingface tutorial.  \n",
    "https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe\n",
    "1. Data pre-processing: Gathering, filtering, splitting and tokenizing.\n",
    "2. Randomly initializing a model\n",
    "3. Config training arguments\n",
    "4. Training\n",
    "5. Verifying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering the data\n",
    "However, training on the full corpus is time- and compute-consuming, and we only need the subset of the dataset concerned with the Python data science stack. So, let’s start by filtering the codeparrot dataset for all files that include any of the libraries in this stack. Because of the dataset’s size, we want to avoid downloading it; instead, we’ll use the streaming feature to filter it on the fly. To help us filter the code samples using the libraries we mentioned earlier, we’ll use the following function:\n",
    "\n",
    "数据太大，进行筛选。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    # Streaming loop\n",
    "    # 处理streaming，这里显式了迭代 streaming 对象的方法，并且添加了 tqdm 进度条\n",
    "    for sample in tqdm(iter(dataset)): \n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v) # add a new value. 将各个 feature: value 添加到字典中\n",
    "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "    return Dataset.from_dict(filtered_dict) # return a Dataset object. 构建Dataset对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will take a very long time to execute, so you should skip it and go to\n",
    "# the next one!\n",
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"  # \"valid\"\n",
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "# Streaming data. 使用streaming的方式加载数据\n",
    "data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "filtered_data = filter_streaming_dataset(data, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the full dataset can take 2-3h depending on your machine and bandwidth. If you don’t want to go through this lengthy process yourself, we provide the filtered dataset on the Hub for you to download:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "# load training and test data separately using arg 'split'.\n",
    "# 直接下载对应的数据，注意数据是分为train 和 validation 的。如果你自己构建数据集，那么需要自己划分\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "# Construct a DatasetDict object\n",
    "# 通过Dataset 构建 DatasetDict\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at an example from the dataset. We’ll just show the first 200 characters of each field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in raw_datasets[\"train\"][0]: # raw_datasets[\"train\"][0] 得到的是一个字典\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "Most documents contain many more than 128 tokens, so simply truncating the inputs to the maximum length would eliminate a large fraction of our dataset. Instead, we’ll use the `return_overflowing_tokens` option to tokenize the whole input and split it into several chunks, as we did in Chapter 6. We’ll also use the `return_length` option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and we’ll get rid of these pieces to avoid padding issues; we don’t really need them as we have plenty of data anyway.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"], # Drop the first two samples. 去前两个样本的内容\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True, # Break a long text into chunks. 将长输入拆分成小块，分块，截断。并不丢弃多余的内容。\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\") # 34 chunks，也就是有34个 chunk\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "# With the overflow_to_sample_mapping field, we can also reconstruct which chunks belonged to which input samples.\n",
    "# 其实就像是 sentence mask一样，就是标记一下这chunk 属于那个字符串\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 .map() 函数批量操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for chunk_size, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if chunk_size == context_length: # drop short sequences. 过滤掉块大小没达到最大的块\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch} # 返回字典\n",
    "# Tokenization, and dropping all the other columns. \n",
    "# 批量tokenize，并且删掉原本数据集中的所有数据。但是他这里这样处理之后，就只剩下 input_ids了，没有 mask\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ Try it out! Getting rid of all the chunks that are smaller than the context size wasn’t a big issue here because we’re using small context windows. As you increase the `context size` (or if you have a corpus of short documents), the fraction of chunks that are thrown away will also grow. A more efficient way to prepare the data is to join all the tokenized samples in a batch with an `eos_token_id` token in between, and then perform the chunking on the concatenated sequences. As an exercise, modify the tokenize() function to make use of that approach. Note that you’ll want to set `truncation=False` and remove the other arguments from the tokenizer to get the full sequence of token IDs.\n",
    "\n",
    "提示：如果上下问窗口太大，那么被删除的信息就越多。这时候可以将文本先tokenize，然后在每个input_ids序列最后添加一个 eos_token_id，然后将大量的样本连接起来。再进行chunking。\n",
    "注意，想要得到完整的文本的ids，需要关掉truncating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing a new model\n",
    "Our first step is to freshly initialize a GPT-2 model. We’ll use the same configuration for our model as for the small GPT-2 model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and pass the bos and eos (beginning and end of sequence) token IDs:\n",
    "\n",
    "如何获得随机初始化的模型? 直接用预训练模型的 config.json 文件初始化一个模型对象就行.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "# Load a GPT-2 configuration file and modify some parameters.\n",
    "# 加载预训练的 GPT-2 模型的配置文件, 并且修改其中的一些参数值, 如vocab_size, 上下文窗口大小 n_ctx等.\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config) # Randomly initialize a model. 随机初始化一个模型\n",
    "model_size = sum(t.numel() for t in model.parameters()) # Compute model parameter numbers. 计算模型参数量\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collator\n",
    "Before we can start training, we need to set up a data collator that will take care of creating the batches. We can use the `DataCollatorForLanguageModeling` collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels — in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we don’t need to duplicate the `input_ids`.\n",
    "\n",
    "Note that `DataCollatorForLanguageModeling` supports both `masked language modeling (MLM)` and `causal language modeling (CLM)`. By default it prepares data for MLM, but we can switch to CLM by setting the argument `mlm=False`:\n",
    "\n",
    "注意, Data collator 的功能很多, 它能够动态padding, 能batching, 还能够创建labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "# DataCollatorForLanguageModeling will generate attention mask according to input token ids. \n",
    "# 加载专门为casual language modeling服务的数据加载器\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# By 'mlm=False', turning off MLM and turning on Casual language modeling. 关闭MLM,打开CLM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing a data collator\n",
    "# 注意这里的数据集只有 input_ids, 因此这个 collator 还能自动产生 mask\n",
    "# data_collator 的输入是一个batch\n",
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)]) \n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "'''\n",
    "input_ids shape: torch.Size([5, 128])\n",
    "attention_mask shape: torch.Size([5, 128])\n",
    "labels shape: torch.Size([5, 128])\n",
    "'''\n",
    "'''\n",
    "⚠️ Shifting the inputs and labels to align them happens inside the model, \n",
    "so the data collator just copies the inputs to create the labels.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "All that’s left to do is configure the training arguments and fire up the Trainer. We’ll use a cosine learning rate schedule with some warmup and an effective batch size of 256 (`per_device_train_batch_size * gradient_accumulation_steps`). Gradient accumulation is used when a single batch does not fit into memory, and incrementally builds up the gradient through several forward/backward passes. We’ll see this in action when we create the training loop with 🤗 Accelerate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds\", # model name and saving directory. 指定模型名字和保存路径\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\", # can also be 'epoch'. 根据步数来决定什么时候评估模型，也可以是 \"epoch\"\n",
    "    eval_steps=5_000, # evaluate the model every 5000 steps. 每 5000 步之后，运行一次 evaluation_strategy \n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8, # Accumulate gradients for parameter updating. 当内存不够装下一个batch的时候，采用累计梯度的方式去计算梯度\n",
    "    num_train_epochs=1, # 训练的epochs，居然只有一次\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n",
    "# There is no metric function. Loss will be computed during evaluation.\n",
    "# 注意这里并没有指定 evaluation 的 metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just start the Trainer and wait for training to finish. Depending on whether you run it on the full or a subset of the training set this will take 20 or 2 hours, respectively, so grab a few coffees and a good book to read!\n",
    "\n",
    "💡 If you have access to a machine with multiple GPUs, try to run the code there. The Trainer automatically manages multiple machines, and this can speed up training tremendously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation with a pipeline\n",
    "\n",
    "Now is the moment of truth: let’s see how well the trained model actually works! We can see in the logs that the loss went down steadily, but to put the model to the test let’s take a look at how well it works on some prompts. To do that we’ll wrap the model in a text generation pipeline, and we’ll put it on the GPU for fast generations if there is one available:\n",
    "\n",
    "添加pipeline的好处是能够自动对模型输出采样，并且能自动启动循环，以保证生成任务的顺利进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# Initialize a pipeline. Local model is also acceptable.\n",
    "# 创建用于文本生成的pipeline，这里注意，因为教程中将模型上传到了huggingface，所以这里的路径是huggingface，完全可以用自己的本地路径\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
