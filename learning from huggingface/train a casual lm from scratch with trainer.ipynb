{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified version of Huggingface tutorial.  \n",
    "https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe\n",
    "1. Data pre-processing: Gathering, filtering, splitting and tokenizing.\n",
    "2. Randomly initializing a model\n",
    "3. Config training arguments\n",
    "4. Training\n",
    "5. Verifying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering the data\n",
    "However, training on the full corpus is time- and compute-consuming, and we only need the subset of the dataset concerned with the Python data science stack. So, letâ€™s start by filtering the codeparrot dataset for all files that include any of the libraries in this stack. Because of the datasetâ€™s size, we want to avoid downloading it; instead, weâ€™ll use the streaming feature to filter it on the fly. To help us filter the code samples using the libraries we mentioned earlier, weâ€™ll use the following function:\n",
    "\n",
    "æ•°æ®å¤ªå¤§ï¼Œè¿›è¡Œç­›é€‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    # Streaming loop\n",
    "    # å¤„ç†streamingï¼Œè¿™é‡Œæ˜¾å¼äº†è¿­ä»£ streaming å¯¹è±¡çš„æ–¹æ³•ï¼Œå¹¶ä¸”æ·»åŠ äº† tqdm è¿›åº¦æ¡\n",
    "    for sample in tqdm(iter(dataset)): \n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v) # add a new value. å°†å„ä¸ª feature: value æ·»åŠ åˆ°å­—å…¸ä¸­\n",
    "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "    return Dataset.from_dict(filtered_dict) # return a Dataset object. æ„å»ºDatasetå¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will take a very long time to execute, so you should skip it and go to\n",
    "# the next one!\n",
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"  # \"valid\"\n",
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "# Streaming data. ä½¿ç”¨streamingçš„æ–¹å¼åŠ è½½æ•°æ®\n",
    "data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "filtered_data = filter_streaming_dataset(data, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the full dataset can take 2-3h depending on your machine and bandwidth. If you donâ€™t want to go through this lengthy process yourself, we provide the filtered dataset on the Hub for you to download:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "# load training and test data separately using arg 'split'.\n",
    "# ç›´æ¥ä¸‹è½½å¯¹åº”çš„æ•°æ®ï¼Œæ³¨æ„æ•°æ®æ˜¯åˆ†ä¸ºtrain å’Œ validation çš„ã€‚å¦‚æœä½ è‡ªå·±æ„å»ºæ•°æ®é›†ï¼Œé‚£ä¹ˆéœ€è¦è‡ªå·±åˆ’åˆ†\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "# Construct a DatasetDict object\n",
    "# é€šè¿‡Dataset æ„å»º DatasetDict\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s look at an example from the dataset. Weâ€™ll just show the first 200 characters of each field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in raw_datasets[\"train\"][0]: # raw_datasets[\"train\"][0] å¾—åˆ°çš„æ˜¯ä¸€ä¸ªå­—å…¸\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "Most documents contain many more than 128 tokens, so simply truncating the inputs to the maximum length would eliminate a large fraction of our dataset. Instead, weâ€™ll use the `return_overflowing_tokens` option to tokenize the whole input and split it into several chunks, as we did in Chapter 6. Weâ€™ll also use the `return_length` option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and weâ€™ll get rid of these pieces to avoid padding issues; we donâ€™t really need them as we have plenty of data anyway.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"], # Drop the first two samples. å»å‰ä¸¤ä¸ªæ ·æœ¬çš„å†…å®¹\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True, # Break a long text into chunks. å°†é•¿è¾“å…¥æ‹†åˆ†æˆå°å—ï¼Œåˆ†å—ï¼Œæˆªæ–­ã€‚å¹¶ä¸ä¸¢å¼ƒå¤šä½™çš„å†…å®¹ã€‚\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\") # 34 chunksï¼Œä¹Ÿå°±æ˜¯æœ‰34ä¸ª chunk\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "# With the overflow_to_sample_mapping field, we can also reconstruct which chunks belonged to which input samples.\n",
    "# å…¶å®å°±åƒæ˜¯ sentence maskä¸€æ ·ï¼Œå°±æ˜¯æ ‡è®°ä¸€ä¸‹è¿™chunk å±äºé‚£ä¸ªå­—ç¬¦ä¸²\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ .map() å‡½æ•°æ‰¹é‡æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for chunk_size, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if chunk_size == context_length: # drop short sequences. è¿‡æ»¤æ‰å—å¤§å°æ²¡è¾¾åˆ°æœ€å¤§çš„å—\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch} # è¿”å›å­—å…¸\n",
    "# Tokenization, and dropping all the other columns. \n",
    "# æ‰¹é‡tokenizeï¼Œå¹¶ä¸”åˆ æ‰åŸæœ¬æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ•°æ®ã€‚ä½†æ˜¯ä»–è¿™é‡Œè¿™æ ·å¤„ç†ä¹‹åï¼Œå°±åªå‰©ä¸‹ input_idsäº†ï¼Œæ²¡æœ‰ mask\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœï¸ Try it out! Getting rid of all the chunks that are smaller than the context size wasnâ€™t a big issue here because weâ€™re using small context windows. As you increase the `context size` (or if you have a corpus of short documents), the fraction of chunks that are thrown away will also grow. A more efficient way to prepare the data is to join all the tokenized samples in a batch with an `eos_token_id` token in between, and then perform the chunking on the concatenated sequences. As an exercise, modify the tokenize() function to make use of that approach. Note that youâ€™ll want to set `truncation=False` and remove the other arguments from the tokenizer to get the full sequence of token IDs.\n",
    "\n",
    "æç¤ºï¼šå¦‚æœä¸Šä¸‹é—®çª—å£å¤ªå¤§ï¼Œé‚£ä¹ˆè¢«åˆ é™¤çš„ä¿¡æ¯å°±è¶Šå¤šã€‚è¿™æ—¶å€™å¯ä»¥å°†æ–‡æœ¬å…ˆtokenizeï¼Œç„¶ååœ¨æ¯ä¸ªinput_idsåºåˆ—æœ€åæ·»åŠ ä¸€ä¸ª eos_token_idï¼Œç„¶åå°†å¤§é‡çš„æ ·æœ¬è¿æ¥èµ·æ¥ã€‚å†è¿›è¡Œchunkingã€‚\n",
    "æ³¨æ„ï¼Œæƒ³è¦å¾—åˆ°å®Œæ•´çš„æ–‡æœ¬çš„idsï¼Œéœ€è¦å…³æ‰truncating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing a new model\n",
    "Our first step is to freshly initialize a GPT-2 model. Weâ€™ll use the same configuration for our model as for the small GPT-2 model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and pass the bos and eos (beginning and end of sequence) token IDs:\n",
    "\n",
    "å¦‚ä½•è·å¾—éšæœºåˆå§‹åŒ–çš„æ¨¡å‹? ç›´æ¥ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ config.json æ–‡ä»¶åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹å¯¹è±¡å°±è¡Œ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "# Load a GPT-2 configuration file and modify some parameters.\n",
    "# åŠ è½½é¢„è®­ç»ƒçš„ GPT-2 æ¨¡å‹çš„é…ç½®æ–‡ä»¶, å¹¶ä¸”ä¿®æ”¹å…¶ä¸­çš„ä¸€äº›å‚æ•°å€¼, å¦‚vocab_size, ä¸Šä¸‹æ–‡çª—å£å¤§å° n_ctxç­‰.\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config) # Randomly initialize a model. éšæœºåˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹\n",
    "model_size = sum(t.numel() for t in model.parameters()) # Compute model parameter numbers. è®¡ç®—æ¨¡å‹å‚æ•°é‡\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collator\n",
    "Before we can start training, we need to set up a data collator that will take care of creating the batches. We can use the `DataCollatorForLanguageModeling` collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels â€” in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we donâ€™t need to duplicate the `input_ids`.\n",
    "\n",
    "Note that `DataCollatorForLanguageModeling` supports both `masked language modeling (MLM)` and `causal language modeling (CLM)`. By default it prepares data for MLM, but we can switch to CLM by setting the argument `mlm=False`:\n",
    "\n",
    "æ³¨æ„, Data collator çš„åŠŸèƒ½å¾ˆå¤š, å®ƒèƒ½å¤ŸåŠ¨æ€padding, èƒ½batching, è¿˜èƒ½å¤Ÿåˆ›å»ºlabels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "# DataCollatorForLanguageModeling will generate attention mask according to input token ids. \n",
    "# åŠ è½½ä¸“é—¨ä¸ºcasual language modelingæœåŠ¡çš„æ•°æ®åŠ è½½å™¨\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# By 'mlm=False', turning off MLM and turning on Casual language modeling. å…³é—­MLM,æ‰“å¼€CLM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing a data collator\n",
    "# æ³¨æ„è¿™é‡Œçš„æ•°æ®é›†åªæœ‰ input_ids, å› æ­¤è¿™ä¸ª collator è¿˜èƒ½è‡ªåŠ¨äº§ç”Ÿ mask\n",
    "# data_collator çš„è¾“å…¥æ˜¯ä¸€ä¸ªbatch\n",
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)]) \n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "'''\n",
    "input_ids shape: torch.Size([5, 128])\n",
    "attention_mask shape: torch.Size([5, 128])\n",
    "labels shape: torch.Size([5, 128])\n",
    "'''\n",
    "'''\n",
    "âš ï¸ Shifting the inputs and labels to align them happens inside the model, \n",
    "so the data collator just copies the inputs to create the labels.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "All thatâ€™s left to do is configure the training arguments and fire up the Trainer. Weâ€™ll use a cosine learning rate schedule with some warmup and an effective batch size of 256 (`per_device_train_batch_size * gradient_accumulation_steps`). Gradient accumulation is used when a single batch does not fit into memory, and incrementally builds up the gradient through several forward/backward passes. Weâ€™ll see this in action when we create the training loop with ğŸ¤— Accelerate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds\", # model name and saving directory. æŒ‡å®šæ¨¡å‹åå­—å’Œä¿å­˜è·¯å¾„\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\", # can also be 'epoch'. æ ¹æ®æ­¥æ•°æ¥å†³å®šä»€ä¹ˆæ—¶å€™è¯„ä¼°æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æ˜¯ \"epoch\"\n",
    "    eval_steps=5_000, # evaluate the model every 5000 steps. æ¯ 5000 æ­¥ä¹‹åï¼Œè¿è¡Œä¸€æ¬¡ evaluation_strategy \n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8, # Accumulate gradients for parameter updating. å½“å†…å­˜ä¸å¤Ÿè£…ä¸‹ä¸€ä¸ªbatchçš„æ—¶å€™ï¼Œé‡‡ç”¨ç´¯è®¡æ¢¯åº¦çš„æ–¹å¼å»è®¡ç®—æ¢¯åº¦\n",
    "    num_train_epochs=1, # è®­ç»ƒçš„epochsï¼Œå±…ç„¶åªæœ‰ä¸€æ¬¡\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n",
    "# There is no metric function. Loss will be computed during evaluation.\n",
    "# æ³¨æ„è¿™é‡Œå¹¶æ²¡æœ‰æŒ‡å®š evaluation çš„ metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just start the Trainer and wait for training to finish. Depending on whether you run it on the full or a subset of the training set this will take 20 or 2 hours, respectively, so grab a few coffees and a good book to read!\n",
    "\n",
    "ğŸ’¡ If you have access to a machine with multiple GPUs, try to run the code there. The Trainer automatically manages multiple machines, and this can speed up training tremendously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation with a pipeline\n",
    "\n",
    "Now is the moment of truth: letâ€™s see how well the trained model actually works! We can see in the logs that the loss went down steadily, but to put the model to the test letâ€™s take a look at how well it works on some prompts. To do that weâ€™ll wrap the model in a text generation pipeline, and weâ€™ll put it on the GPU for fast generations if there is one available:\n",
    "\n",
    "æ·»åŠ pipelineçš„å¥½å¤„æ˜¯èƒ½å¤Ÿè‡ªåŠ¨å¯¹æ¨¡å‹è¾“å‡ºé‡‡æ ·ï¼Œå¹¶ä¸”èƒ½è‡ªåŠ¨å¯åŠ¨å¾ªç¯ï¼Œä»¥ä¿è¯ç”Ÿæˆä»»åŠ¡çš„é¡ºåˆ©è¿›è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# Initialize a pipeline. Local model is also acceptable.\n",
    "# åˆ›å»ºç”¨äºæ–‡æœ¬ç”Ÿæˆçš„pipelineï¼Œè¿™é‡Œæ³¨æ„ï¼Œå› ä¸ºæ•™ç¨‹ä¸­å°†æ¨¡å‹ä¸Šä¼ åˆ°äº†huggingfaceï¼Œæ‰€ä»¥è¿™é‡Œçš„è·¯å¾„æ˜¯huggingfaceï¼Œå®Œå…¨å¯ä»¥ç”¨è‡ªå·±çš„æœ¬åœ°è·¯å¾„\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
