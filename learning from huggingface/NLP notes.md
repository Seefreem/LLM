# pipeline
Pipeline ä½œä¸ºHuggingfaceçš„åŸºç¡€å·¥å…·ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°å¸®åŠ©æˆ‘ä»¬ç›´æ¥ä½¿ç”¨huggingfaceä¸Šçš„æ¨¡å‹ã€‚
å½“æˆ‘ä»¬å°†è‡ªå·±çš„æ¨¡å‹è£…è¿›Pipelineåï¼Œä¹Ÿæ–¹ä¾¿æˆ‘ä»¬ä½¿ç”¨Huggingfaceçš„å…¶ä»–åŠŸèƒ½ï¼Œæ¯”å¦‚ evaluator ã€‚  
Pipeline is an abstract of the pre-process, modeling and post-process.

Some of the currently available pipelines are:  
- feature-extraction (get the vector representation of a text)
- fill-mask
- ner (named entity recognition)
- question-answering
- sentiment-analysis
- summarization
- text-generation
- translation
- zero-shot-classification
è¿™äº›ä¹Ÿæ˜¯åˆ›å»ºpipelineæ—¶éœ€è¦è¾“å…¥çš„å…³é”®å­—ã€‚

`Zero-shot classification` æ¯”è¾ƒæœ‰æ„æ€ã€‚å®ƒæ˜¯æŒ‡åœ¨ä¸ç»è¿‡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œç»™textæ‰“ä¸Šæ–°çš„æ ‡ç­¾ã€‚æ¯”å¦‚ candidate_labels=["education", "politics", "business"]ã€‚è¿™æ ·çš„å¥½å¤„æ˜¯ï¼Œä¸ç”¨äººä¸ºæ ‡æ³¨æ•°æ®ï¼ŒåŠ å¿«æ•°æ®ç”Ÿæˆå’Œæ”¶é›†çš„è¿‡ç¨‹ã€‚å½“ç„¶æœ€å¥½è¿˜æ˜¯äººä¸ºå®¡æ ¸ä¸€ä¸‹ã€‚

å¯¹äº `Text generation` ä½ å¯ä»¥æ§åˆ¶ç”Ÿæˆå¤šå°‘ä¸ªå¤‡é€‰å¥å­ï¼ˆnum_return_sequencesï¼‰ï¼Œè¿˜å¯ä»¥è®¾ç½®ç”Ÿæˆçš„å¥å­çš„é•¿åº¦ï¼ˆmax_lengthï¼Œæ•´ä¸ªè¾“å‡ºå¥å­çš„tokenæ•°ï¼‰ã€‚
åœ¨åˆ›å»ºpipelineçš„æ—¶å€™ä¹Ÿå¯ä»¥é€šè¿‡å…³é”®å­—model æŒ‡å®šæ¨¡å‹ã€‚  
å…¶å® pipeline ä½œä¸ºæ•´ä¸ªæµç¨‹çš„æŠ½è±¡ï¼Œé‚£ä¹ˆå®ƒä¹Ÿæ˜¯æ”¯æŒæ•´ä¸ªæµç¨‹ä¸­æ‰€éœ€è¦çš„å‚æ•°çš„ã€‚ ç„¶åå› ä¸ºè¿™å±‚æŠ½è±¡å¯èƒ½åŒ…å«é”™è¯¯çš„è¾“å…¥ã€‚æ‰€ä»¥åœ¨å­æ¨¡å—ä¸­è‚¯å®šæ˜¯æœ‰é²æ£’æ€§æ£€æµ‹çš„ã€‚

é€šè¿‡ Inference API æ‰€æœ‰çš„æ¨¡å‹éƒ½å¯ä»¥åœ¨huggingfaceçš„ç½‘é¡µä¸Šè¿›è¡Œè¯•ç”¨ã€‚

åœ¨mask fillingä¾‹å­ä¸­ï¼Œ<mask>å…³é”®å­—è¡¨ç¤ºä¸€ä¸ª mask tokenã€‚ä½†æ˜¯ä¸åŒçš„æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨ä¸åŒçš„tokenï¼Œå› æ­¤æœ‰å¿…è¦æ£€æŸ¥ä¸€ä¸‹ã€‚æ£€æŸ¥çš„æ–¹æ³•å¾ˆå¤šï¼Œå¯ä»¥çœ‹ä¾‹å­ï¼Œå¯ä»¥çœ‹åŸå§‹è®ºæ–‡ã€‚

æ¯ä¸ªä¸åŒçš„ä»»åŠ¡æˆ–è€…æ¨¡å‹å¯èƒ½éƒ½ä¼šæœ‰å®ƒä»¬ç‰¹å®šçš„å‚æ•°ï¼Œæ‰€ä»¥ä½¿ç”¨å‰è®°å¾—æ£€æŸ¥ã€‚

å¯¹äº"question-answering"pipelineï¼Œä»–ä»¬æ‰§è¡Œçš„ç»“æœæ˜¯ä»context ä¸­æŠ½å–ç­”æ¡ˆï¼Œè€Œä¸æ˜¯åŸºäºcontextç”Ÿæˆç­”æ¡ˆã€‚
Note that this pipeline works by extracting information from the provided context; it does not generate the answer.


# Transformer models
Broadly, they can be grouped into three categories:  
- GPT-like (also called auto-regressive Transformer models)
- BERT-like (also called auto-encoding Transformer models)
- BART/T5-like (also called sequence-to-sequence Transformer models)

This type of model develops a statistical understanding of the language it has been trained on, but itâ€™s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called transfer learning. During this process, the model is fine-tuned in a supervised way â€” that is, using human-annotated labels â€” on a given task.

An example of a task is predicting the next word in a sentence having read the n previous words. This is called causal language modeling because the output depends on the past and present inputs, but not the future ones.  

Each of these parts can be used independently, depending on the task:

- Encoder-only models: Good for tasks that require `understanding` of the input, such as sentence classification and named entity recognition. -- Gemini
- Decoder-only models: Good for `generative` tasks such as text generation. --GPT-x
- Encoder-decoder models or `sequence-to-sequence` models: Good for `generative` tasks that require an input, such as translation or summarization.

åœ¨è®­ç»ƒs2sæ¨¡å‹çš„æ—¶å€™ï¼Œå¯¹äºdecoderæ¥è¯´è¿˜æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„å­¦ä¹ ï¼Œå› ä¸ºå®ƒè¿˜æ˜¯åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚  
ä½†æ˜¯æˆ‘ä»¬å¯èƒ½ä¼šå°†è¿™ä¸ªè®­ç»ƒç†è§£ä¸ºç›‘ç£å­¦ä¹ ï¼Œå› ä¸ºæˆ‘ä»¬æä¾›äº†ä¸¤ä¸ªsequencesã€‚  
To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words

Terminology:
- Architecture: This is the skeleton of the model â€” the definition of each layer and each operation that happens within the model.
- Checkpoints: These are the weights that will be loaded in a given architecture.
- Model: This is an umbrella term that isnâ€™t as precise as â€œarchitectureâ€ or â€œcheckpointâ€: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.

åœ¨Sequence-2-sequence çš„ä¾‹å­ä¸­ï¼Œä»–ä»¬ä¹Ÿæåˆ°äº†ï¼Œä½ å¯ä»¥åœ¨è¿™ä¸ªarchitectureä¸­åŠ è½½ä¸åŒçš„ENcoderå’ŒDecoderæ¥æ»¡è¶³ä½ çš„ç‰¹å®šçš„ä»»åŠ¡éœ€æ±‚ã€‚

Summary:  
Model	            Examples	                                Tasks
Encoder	            ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa	Sentence classification, named entity recognition,          
                                                                extractive question answering
Decoder	            CTRL, GPT, GPT-2, Transformer XL	        Text generation
Encoder-decoder     BART, T5, Marian, mBART	                    Summarization, translation, generative question answering

# behind the pipeline
```python 
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
``` 
è¿™é‡Œæˆ‘ä»¬é€šè¿‡ return_tensors æŒ‡å®šè¿”å›ä»€ä¹ˆç±»å‹çš„ tensorï¼Œ pt æŒ‡çš„æ˜¯pytorchã€‚
Donâ€™t worry about padding and truncation just yet; weâ€™ll explain those later. The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result).

pythonæ¯”è¾ƒå¥½çš„ä¸€ç‚¹æ˜¯ï¼Œå®ƒå…·æœ‰éå¸¸ä¸°å¯Œçš„åº“ï¼Œèƒ½å¤Ÿå®ç°å¾ˆå¤šåŠŸèƒ½ï¼Œå¹¶ä¸”ç®€å•æ˜“ä¸Šæ‰‹ï¼ˆå› ä¸ºå®ƒèƒ½è‡ªåŠ¨å¤„ç†å¾ˆå¤šäº‹æƒ…ï¼‰ã€‚  
ä½†æ˜¯è¿™ä¹Ÿæ˜¯pythonçš„éº»çƒ¦ä¹‹å¤„ã€‚ä¸°å¯Œ+è‡ªåŠ¨æ„å‘³ç€éšè—çš„å¤æ‚åº¦å¾ˆé«˜ï¼Œå¾ˆå¤šæ—¶å€™äº§ç”Ÿçš„ç»“æœå¹¶ä¸æ˜¯ä½ æƒ³è¦çš„ã€‚å¹¶ä¸”ä½ è¿˜ä¸çŸ¥é“ã€‚ç„¶åå¾ˆå¤šæ—¶å€™ä½ éœ€è¦æ³¨æ„ç±»å‹ï¼Œå› ä¸ºç±»å‹çš„ä¸åŒä¼šå¯¼è‡´å¾ˆå¤šçš„é”™è¯¯ã€‚  

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¾€å¾€æ¨¡å‹åœ¨å‘å¸ƒçš„æ—¶å€™éƒ½æ˜¯ç§»é™¤äº†task-specific headï¼Œå› æ­¤ä½ åœ¨ä½¿ç”¨çš„æ—¶å€™è¦é¦–å…ˆåˆ¤æ–­ä½ ä¸‹è½½çš„æ¨¡å‹æ˜¯å¦åŒ…å«äº†headã€‚  
æ€ä¹ˆçœ‹ï¼Ÿ  
å¦‚æœæœ‰å¯¹åº”çš„æ¨¡å‹çš„å®šä¹‰ï¼Œé‚£ä¹ˆç›´æ¥çœ‹æºç ï¼Œå¦‚æœæ²¡æœ‰ï¼Œé‚£ä¹ˆå°±çœ‹çœ‹model cardï¼Œçœ‹ç¤ºä¾‹ã€‚å¦‚æœè¿˜æ²¡æœ‰ï¼Œé‚£ä¹ˆå°±å¯ä»¥ç›´æ¥æ‰“å°æ¨¡å‹çš„ç»“æ„ï¼ˆä¹Ÿå°±æ˜¯ç›´æ¥printæ¨¡å‹ï¼‰ï¼Œå†ä¸è¡Œå°±ç›´æ¥çœ‹æ¨¡å‹çš„è¾“å‡ºã€‚  
ä¸€èˆ¬æ¥è¯´ï¼Œæ²¡æœ‰headçš„åçš„è¾“å‡ºæ ¼å¼æ˜¯ [batch, sequence length, model dimension]

ä¸€èˆ¬æ¥è¯´ï¼Œtransformeræ¨¡å‹éƒ½æ˜¯åŒ…å«äº†embedding modelçš„ã€‚ä½ ä¸ç”¨åˆ»æ„å»æ‰§è¡Œembeddingï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨è®¡ç®—ã€‚   

ä¸ºäº†èƒ½æ–¹ä¾¿çš„åŠ è½½æ¨¡å‹å¹¶ä¸”åº”ç”¨äºä¸åŒçš„ä»»åŠ¡ï¼ŒTransformersæä¾›äº†é’ˆå¯¹ä¸åŒä»»åŠ¡çš„æŠ½è±¡ç±»ï¼š 
* Model (retrieve the hidden states)
* ForCausalLM
* ForMaskedLM
* ForMultipleChoice
* ForQuestionAnswering
* ForSequenceClassification
* ForTokenClassification

ä»¥ AutoModelForSequenceClassification ä¸ºä¾‹å­ï¼ŒåŠ è½½çš„æ¨¡å‹ä¸­å¤šäº†ä¸€å±‚çº¿æ€§å…¨è¿æ¥å±‚ï¼Œä½†æ˜¯æ³¨æ„å…¨è¿æ¥å±‚çš„è¾“å‡ºæ˜¯logitsï¼Œå¹¶ä¸”è¿™ä¸ªå…¨è¿æ¥å±‚æ˜¯çº¿æ€§çš„ï¼Œä¸å¸¦æ¿€æ´»å‡½æ•°ã€‚è¿™é‡Œæ¯”è¾ƒä¸ä¸€æ ·çš„æ˜¯ï¼Œheadçš„æœ€åä¸€å±‚å¾€å¾€éƒ½ä¸å¸¦æ¿€æ´»å‡½æ•°ã€‚


è¿˜æœ‰éœ€è¦æ³¨æ„çš„æ˜¯åœ¨Transformersè¿™ä¸ªåº“ä¸­ï¼Œæ¨¡å‹çš„è¾“å‡ºå¾€å¾€éƒ½æ˜¯ä¸€ä¸ªç±»ä¼¼äºå­—å…¸çš„æ•°æ®ç»“æ„ï¼Œå› æ­¤ä½ å¾€å¾€æ˜¯é€šè¿‡å…³é”®è¯è¿›è¡Œè®¿é—®æ•°æ®ã€‚    
Note that the outputs of ğŸ¤— Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs["last_hidden_state"]), or even by index if you know exactly where the thing you are looking for is (outputs[0]).

## about model
æ¨¡å‹ä¸€èˆ¬å…·æœ‰ä¸¤ä¸ªé‡è¦æ–‡ä»¶ï¼š configuration + checkpoint  
å½“æˆ‘ä»¬æ‰§è¡Œ from_pretrained("bert-base-cased") çš„æ—¶å€™ï¼Œå®é™…ä¸Šæ‰§è¡Œçš„æ˜¯å…ˆæ ¹æ® configuration æ–‡ä»¶åˆ›å»º config å¯¹è±¡ï¼Œç„¶ååˆ›å»ºæ¨¡å‹çš„ç±»ï¼Œå¹¶ä¸”å®ä¾‹åŒ–ï¼Œæœ€åå°±æ˜¯åŠ è½½é¢„è®­ç»ƒçš„å‚æ•°ã€‚  
è¿™ä¸ªè¿‡ç¨‹å¯ä»¥æˆ‘ä»¬è‡ªå·±å†™ï¼Œå¯ä»¥æ‰“æ–­ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ä¿®æ”¹é…ç½®æ–‡ä»¶æ¥ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼Œç„¶åä»å¤´å¼€å§‹è®­ç»ƒã€‚  

åˆ›å»ºä¸€ä¸ªæ¨¡å‹å¯¹è±¡æœ‰ä¸¤ç§æ–¹å¼ï¼ŒAutoModel æˆ–è€… ç‰¹å®šçš„æ¨¡å‹åã€‚  
é€šè¿‡ç‰¹å®šæ¨¡å‹ååˆ›å»ºæ¨¡å‹çš„ä¾‹å­ï¼š
```python
from transformers import BertConfig, BertModel
config = BertConfig()
model = BertModel(config) # Model is randomly initialized!

# æˆ–è€…
from transformers import BertModel
model = BertModel.from_pretrained("bert-base-cased") # ç­‰æ•ˆäº AutoModel.from_pretrained("bert-base-cased")

```

The weights have been downloaded and cached (so future calls to the from_pretrained() method wonâ€™t re-download them) in the cache folder, which defaults to `~/.cache/huggingface/transformers`. You can customize your cache folder by setting the `HF_HOME` environment variable.

The `pytorch_model.bin` file is known as the state dictionary; it contains all your modelâ€™s weights. The two files go hand in hand; the `configuration` is necessary to know your modelâ€™s architecture, while the model weights are your modelâ€™s parameters.

## tokenizer
æ³¨æ„ï¼šç›´æ¥ä½¿ç”¨tokenizer(sentences, max_length=1024, return_tensors="pt")å°±å¥½ã€‚ä¸è¦è‡ªå·±å»å†™è¿™ä¸ªè¿‡ç¨‹ï¼Œä¸è¦ä½¿ç”¨ .tokenize(sequence) è¿™äº›å­å‡½æ•°ã€‚å› ä¸ºåƒæ·»åŠ ç‰¹æ®Štokenã€paddingå’Œtruncation ç­‰åŠŸèƒ½éƒ½æ²¡æœ‰åœ¨å­å‡½æ•°ä¸­å®ç°ã€‚å› æ­¤ç›´æ¥ä½¿ç”¨å­å‡½æ•°è‚¯å®šæ˜¯æœ‰é—®é¢˜çš„ã€‚  
tokenization çš„æ–¹æ³•æœ‰å¾ˆå¤šï¼Œä¸»è¦å¯ä»¥åˆ†ä¸ºï¼šword-basedã€Character-based å’Œ Subword tokenizationã€‚
Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.

tokenizerçš„ä½¿ç”¨åŠæ³•ä¹ŸæŒºå¤šçš„ã€‚
ä½ å¯ä»¥ç›´æ¥ä¸€æ­¥åˆ°ä½ï¼š
```python
doc1_inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors="pt")
```
ä¹Ÿå¯ä»¥ä¸€æ­¥ä¸€æ­¥æ±‚è§£ï¼š
```python
sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
```

```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
# =====================================================================
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```
```python
model_inputs = tokenizer(sequences, padding="max_length", max_length=4, truncation=True)
```
å¯¹äºä¸Šé¢çš„è¿™è¡Œä»£ç ï¼Œå¦‚æœå­—ç¬¦ä¸²é•¿åº¦è¶…è¿‡äº† max_lengthï¼Œé‚£ä¹ˆå°±åªä¼šä¿ç•™å‰é¢çš„é‚£éƒ¨åˆ†ï¼Œè€Œç›´æ¥ä¸¢å¼ƒåé¢çš„éƒ¨åˆ†ã€‚è¿™ä¸ªéšè—çš„é—®é¢˜å¯èƒ½ä¼šå¯¼è‡´æ„æƒ³ä¸åˆ°çš„ç»“æœã€‚


## processing the data--tokenization, batching and loading
å¯¹äºåƒBERTè¿™ç§èƒ½åˆ¤æ–­ä¸¤ä¸ªå¥å­ä¹‹é—´çš„å…³ç³»çš„æ¨¡å‹ã€‚å®ƒçš„tokenizer å¯ä»¥æ¥å—ä¸¤ä¸ªå­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥æ•°æ®ï¼š
```python
inputs = tokenizer("This is the first sentence.", "This is the second one.")
```
å½“ç„¶è¿™ä¸¤ä¸ªå¥å­ä¹Ÿå¯ä»¥æ˜¯ä¸¤ä¸ªç­‰é•¿çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
tokenizerä¼šå°†è¿™ä¸¤ä¸ªå­—ç¬¦ä¸²è¿æ¥èµ·æ¥ï¼Œå¹¶æ·»åŠ å¯¹åº”çš„ç‰¹æ®Šå­—ç¬¦ã€‚
ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™æ—¶å€™å¹¶ä¸ä¼šè¿›è¡Œpaddingã€‚å¦‚æœæ¨¡å‹éœ€è¦token_type_ids çš„è¯ï¼Œtokenizerä¹Ÿä¼šè‡ªåŠ¨ç”Ÿæˆå¯¹åº”çš„ token_type_idsã€‚
å› ä¸ºä¸Šé¢æ²¡æœ‰æŒ‡å®š paddingï¼Œ æ‰€ä»¥ï¼Œå¯èƒ½å¹¶ä¸ä¼šè¿›è¡Œpaddingï¼Œè¿™æ—¶å€™paddingå¯èƒ½å¹¶ä¸æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚å› ä¸ºè¿™æ—¶å€™ä¼šé€‰æ‹©æœ€é•¿çš„å¥å­ä½œä¸ºpaddingçš„æ ‡å‡†ã€‚
paddingå¯ä»¥åœ¨è¾“å…¥ç»™æ¨¡å‹çš„æ—¶å€™åŠ¨æ€è¿›è¡Œã€‚

å½“æ•°æ®é›†æ¯”è¾ƒå¤§çš„æ—¶å€™ï¼Œå°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­å¹¶è¿›è¡Œå¤„ç†å¯èƒ½ä¼šé‡åˆ°å†…å­˜å¤§å°ç“¶é¢ˆã€‚å› æ­¤ä½¿ç”¨ Dataset.map()  å¯ä»¥æœ‰æ•ˆè¾ƒå°‘å†…å­˜çš„éœ€æ±‚ã€‚

It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:
```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
æ³¨æ„è¿™é‡Œåœ¨è¾“å…¥æ•°æ®ç»™data collatorçš„æ—¶å€™ï¼Œæ•°æ®å¾—æ˜¯æ•°å­—ï¼Œä¸èƒ½æ˜¯å­—ç¬¦ä¸²ä»€ä¹ˆçš„ã€‚å¹¶ä¸”è¿™ä¸€æ­¥æ˜¯åŸºäºtokenizeä¹‹åçš„ã€‚
æ³¨æ„ data collatorçš„ä½œç”¨æ˜¯ç»™è¾“å…¥çš„batch è‡ªåŠ¨æ·»åŠ  paddingã€‚è€Œä¸æ˜¯è‡ªåŠ¨ç”Ÿæˆbatch
```python
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```
ä½†æ˜¯å•Šï¼Œä»–è¿™é‡Œè¿˜æ˜¯æ²¡è®²æ€ä¹ˆæ„å»ºåŸºäº bath çš„ data loaderã€‚å¯èƒ½è¿˜æ˜¯å¾—å›åˆ°pytorchã€‚ æˆ–è€…å¾—çœ‹çœ‹huggingfaceçš„ datadict ç±»ã€‚

Note that when you pass the tokenizer as we did here, the default data_collator used by the Trainer will be a DataCollatorWithPadding as defined previously, so you can skip the line data_collator=data_collator in this call. It was still important to show you this part of the processing in section 2!
åˆæ¥äº†ï¼Œåˆæœ‰ä¸€äº›é»˜è®¤çš„å¤„ç†ã€‚
å¹¶ä¸”åœ¨å†™æ•´ä½“çš„Trainerçš„ä»£ç çš„æ—¶å€™ï¼Œå¹¶æ²¡æœ‰ç§»é™¤å¤šä½™çš„å±æ€§ã€‚
å¹¶ä¸”è¿™é‡Œé¢ä¼¼ä¹å¿½ç•¥äº†batch çš„é—®é¢˜ã€‚æˆ–è€…è¯´ï¼Œå¹¶æ²¡æœ‰æä¾›è®¾ç½®batchçš„é€‰é¡¹ã€‚
æ‰€ä»¥è¿™é‡Œæœ‰å¤šç§è§£å†³æ–¹å¼ï¼Œç¬¬ä¸€æ˜¯ç»§ç»­æ²¿ç”¨Trainerï¼Œä½†æ˜¯è‡ªå·±å®ç°æ¨¡å‹çš„æ¨ç†å‡½æ•°ã€‚ç¬¬äºŒæ˜¯è‡ªå·±æ„å»ºè®­ç»ƒè¿‡ç¨‹ã€‚
åœ¨åé¢è®²äº†ï¼Œå®é™…ä¸Šè¿˜æ˜¯é‡‡ç”¨äº†torchçš„dataloader


è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥é€šè¿‡ .predict() å‡½æ•°è¿›è¡Œé¢„æµ‹ã€‚å¾—åˆ°çš„ç»“æœæ˜¯å­—å…¸ï¼ŒåŒ…å«äº† predictions, label_ids, and metrics ç­‰ä¿¡æ¯ã€‚ä½†è¯·æ³¨æ„ï¼Œ all Transformer models return logitsã€‚æ‰€ä»¥æ‹¿åˆ°è¾“å‡ºåè¿˜å¾—è¿›è¡Œè½¬æ¢ã€‚

```python
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```
è¿™é‡Œæåˆ°äº†å‡ ä¸ªå…³äº datasetdictçš„å‡ ä¸ªå¸¸ç”¨çš„å‡½æ•°ã€‚
ç„¶ååŸºäºä¸Šé¢çš„ç»“æœï¼Œé€šè¿‡ torch çš„dataloader æ¥batchåŒ– (ä½†æ˜¯æ‰‹å†Œåˆè¯´ï¼Œè¿™äº›å®é™…ä¸Šéƒ½åœ¨Trainerä¸­å®ç°äº†ï¼Œè¿™å¬èµ·æ¥æ¯”è¾ƒåˆç†ã€‚å¯èƒ½è¿˜å¾—çœ‹çœ‹æºç )ï¼š
å…¶å®æŒ‰ç…§
```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)

```
All ğŸ¤— Transformers models will return the loss when labels are provided. è¿™æŒºå¥½ã€‚

â€œA full trainingâ€ è¿™ä¸€ç« å¾ˆé‡è¦ã€‚è®²äº†å¾ˆå¤šæœ‰ç”¨çš„çŸ¥è¯†ã€‚


# Dataset
load_dataset() æ”¯æŒåŠ è½½æœ¬åœ°çš„å¤šç§æ–‡ä»¶ã€‚å¹¶ä¸”æ”¯æŒä¸€æ¬¡æ€§åŠ è½½å¤šä¸ªæ–‡ä»¶ï¼Œè¿”å›DatasetDict å¯¹è±¡ã€‚
è¿˜æ”¯æŒè§£å‹gzip, ZIP and TAR æ–‡ä»¶ã€‚
è¿˜æ”¯æŒé€šè¿‡URLåŠ è½½æ•°æ®ï¼ˆstreamingæŠ€æœ¯ï¼‰ã€‚
The data_files argument of the load_dataset() function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by setting data_files="*.json"). See the ğŸ¤— Datasets documentation for more details.

Now that weâ€™ve got a dataset to play with, letâ€™s get our hands dirty with various `data-wrangling` techniques!

é‡ç£…ï¼ï¼ï¼éšæœºè·å–ä¸€å°éƒ¨åˆ†æ•°æ®ç”¨äºæ„å»ºpipelineçš„æ–¹æ³•å¦‚ä¸‹ï¼š
```python
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# Peek at the first few examples
drug_sample[:3]

```
è¿™é‡Œæ¶‰åŠåˆ°datasetDict ç±»çš„ä¸¤ä¸ªåŠŸèƒ½ï¼šshuffleå’Œselectï¼ˆä¹±åºå’ŒæŠ½å–ï¼‰ï¼ˆslicing and dicingï¼‰
Dataset.select() expects an iterable of indices, so weâ€™ve passed range(1000) to grab the first 1,000 examples from the shuffled dataset.
From this sample we can already see a few `quirks` in our datasetã€‚

Dataset.unique(column name)  è¿”å›å»é‡ä¹‹åçš„å…ƒç´ åˆ—è¡¨ã€‚

new_dataset = DatasetDict.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
è¿™ä¸ªæ–¹æ³•å¯ä»¥ç”¨äºé‡å‘½å åˆ—ã€‚ä½†æ˜¯æ³¨æ„éœ€è¦ç”¨ä¸€ä¸ªæ–°çš„å¯¹è±¡æ¥æ¥ä½è¿”å›å€¼ã€‚

é€šç”¨ é€šè¿‡ .map() å‡½æ•°ï¼Œèƒ½å¤Ÿå®ç°å„ç§æ•°æ®ä¿®æ”¹æ“ä½œã€‚

Dataset.filter() å‡½æ•°ç”¨äºç­›é€‰è¡Œæ•°æ®ã€‚è¿™ä¸ªå‡½æ•°å’Œmap()å‡½æ•°çš„è¿è¡Œæ–¹å¼ç±»ä¼¼ï¼Œåªä¸è¿‡ä¸€æ¬¡åªæ”¯æŒä¸€ä¸ªæ ·æœ¬ã€‚å¹¶ä¸”æ¥å—çš„å‡½æ•°çš„è¿”å›å€¼ä¸å†æ˜¯å­—å…¸ï¼Œè€Œæ˜¯boolean valueã€‚
å¸¸è§çš„ç”¨æ³•æ˜¯å’Œlambdaå‡½æ•°ç»“åˆï¼š  
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None) # æœ‰äº›æ•°æ®è¡Œæ˜¯ç©ºçš„ï¼Œè¿™é‡Œç”¨äºå–æ‰ç©ºè¡Œã€‚

ç»éªŒï¼š
Whenever youâ€™re dealing with customer reviews, a good practice is to check the number of words in each review. A review might be just a single word like â€œGreat!â€ or a `full-blown` essay with thousands of words, and depending on the use case youâ€™ll need to handle these extremes differently. To compute the number of words in each review, weâ€™ll use a rough `heuristic` based on splitting each text by whitespace.
Letâ€™s define a simple function that counts the number of words in each review:

```python
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```
é€šè¿‡è¿”å›æ–°çš„ keyçš„æ–¹å¼ï¼Œå¯ä»¥åˆ›å»ºæ–°çš„åˆ—ï¼Œå¹¶ä¸”å®ç°èµ‹å€¼ã€‚å¦å¤–è¿˜å¯ä»¥ä½¿ç”¨ Dataset.add_column() æ·»åŠ æ–°çš„åˆ—ï¼Œç”¨æ³•å’Œpandas ä¸€æ ·ã€‚

drug_dataset["train"].sort("review_length")[:3]
è¿™ä¸ªsort å‡½æ•°èƒ½å¯¹åˆ—ä¸­çš„æ‰€æœ‰å…ƒç´ è¿›è¡Œæ’åºã€‚ä½†æ˜¯è¿™é‡Œæœ‰ä¸ªé—®é¢˜ï¼Œè¿™ä¸ªæ’åºæ˜¯in-placeæ’åºè¿˜æ˜¯è¿”å›ä¸€ä¸ªæ–°çš„å¯¹è±¡ï¼Ÿ


The last thing we need to deal with is the presence of HTML character codes in our reviews. We can use Pythonâ€™s html module to unescape these characters, like so:
```python
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
Copied
"I'm a transformer called BERT"
# Weâ€™ll use Dataset.map() to unescape all the HTML characters in our corpus:

drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

Dataset.map() çš„batched å‚æ•°é»˜è®¤æƒ…å†µä¸‹çš„batch size æ˜¯1000. åŠ é€Ÿæ–¹æ³•æ˜¯ list comprehension
å¯ä»¥é€šè¿‡ batch_size å‚æ•°æ”¹å˜batch å¤§å°ã€‚åªä¸è¿‡è¿™ä¸ªbatch sizeæ˜¯æ‰§è¡Œmapçš„å¤§å°ï¼Œè€Œä¸æ˜¯è¾“å…¥ç»™æ¨¡å‹çš„batch size

This means that using a fast tokenizer with the batched=True option is 30 times faster than its slow counterpart with no batching â€” this is truly amazing! Thatâ€™s the main reason why fast tokenizers are the default when using AutoTokenizer (and why they are called â€œfastâ€).
å…³äºtokenizationçš„åŠ é€Ÿé—®é¢˜ï¼šé¦–å…ˆæ˜¯è®¾ç½® batched=Trueï¼Œç„¶åæ˜¯ä½¿ç”¨ fast Tokenizerï¼Œä½†æ˜¯å¹¶ä¸æ˜¯æ‰€æœ‰çš„tokenizeréƒ½æœ‰fastç‰ˆæœ¬
å¦‚æœ batched=True åˆ™ä¼ é€’ç»™å›è°ƒå‡½æ•°çš„å€¼æ˜¯ä¸€ä¸ªå­è¡¨ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªå±æ€§ä¸‹çš„å…ƒç´ ç±»å‹æ˜¯listã€‚

```python
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)
def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)
tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```
ä½¿ç”¨ æ®è¯´ä¹Ÿèƒ½åŠ é€Ÿï¼Œä½†æ˜¯æˆ‘åœ¨å®è·µçš„è¿‡ç¨‹ä¸­ä¼¼ä¹å¹¶æ²¡æœ‰ä½“ä¼šåˆ°ã€‚
In general, we donâ€™t recommend using Python multiprocessing for fast tokenizers with batched=True.
å®é™…ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œè®¾ç½®batched=True å°±å¤Ÿäº†ã€‚

Datasets is designed to be interoperable with libraries such as Pandas, NumPy, PyTorch, TensorFlow, and JAX. Letâ€™s take a look at how this works.
Datasets ç±»èƒ½å°†æ•°æ®è½¬åŒ–ä¸ºpandasã€numpyç­‰å…¶ä»–ç±»ã€‚

è®¾ç½®ç±»å‹è½¬æ¢çš„æ–¹å¼ä¹Ÿå¾ˆç®€å•ï¼š Dataset.set_format('pandas')
This function only changes the output format of the dataset, so you can easily switch to another format without affecting the underlying data format, which is Apache Arrow.
ç„¶åå°±å¯ä»¥è¿™æ ·ä½¿ç”¨äº†ï¼š
drug_dataset["train"][:3]
å¦ˆå‘€ï¼Œpython å·²ç»è¢«ç©å¾—å®Œå…¨å˜äº†æ ·äº†ã€‚ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿä¸å­˜åœ¨çš„ã€‚
ğŸš¨ Under the hood, Dataset.set_format() changes the return format for the datasetâ€™s __getitem__() dunder method. This means that when we want to create a new object like train_df from a Dataset in the "pandas" format, we need to slice the whole dataset to obtain a pandas.DataFrame. You can verify for yourself that the type of drug_dataset["train"] is Dataset, irrespective of the output format.
ä½†æ˜¯æ—¢ç„¶è¿”å›çš„æ˜¯pandas å¯¹è±¡ï¼Œé‚£ä¹ˆå°±èƒ½çº§è”è°ƒç”¨pandasçš„æ–¹æ³•ã€‚

ä»pandasåˆ°datasetsï¼š
freq_dataset = Dataset.from_pandas(frequencies)

This `wraps up` our tour of the various preprocessing techniques available in ğŸ¤— Datasets. To `round out` the section, letâ€™s create a validation set to prepare the dataset for training a classifier on. Before doing so, weâ€™ll reset the output format of drug_dataset from "pandas" to "arrow":
drug_dataset.reset_format() # è®¾ç½®å›é»˜è®¤çš„è¾“å‡ºæ ¼å¼

æ„è§åˆ’åˆ†æ•°æ®ï¼Œå°†æ•°æ®åˆ’åˆ†æˆå¤šä»½ï¼Œåˆ’åˆ†æµ‹è¯•é›†ï¼Œåˆ’åˆ†éªŒè¯é›†ï¼š
```python
drug_dataset_clean = drug_dataset["train"].train_test_split(train_size=0.8, seed=42)
# Rename the default "test" split to "validation"
drug_dataset_clean["validation"] = drug_dataset_clean.pop("test")
# Add the "test" set to our `DatasetDict`
drug_dataset_clean["test"] = drug_dataset["test"]
drug_dataset_clean
```

æ³¨æ„ï¼šDatasets will cache every downloaded dataset and the operations performed on itã€‚ç€äº†å¯èƒ½å‡ºç°å‘ã€‚
ä½¿ç”¨æ•°æ®å‰æ£€æŸ¥æ•°æ®æ€»æ˜¯å¥½çš„ã€‚

æ³¨æ„å¦‚æœè¦ä¿å­˜ä¸ºJSONæ–‡ä»¶çš„è¯ï¼Œæ¯ä¸ªæ•°æ®é›†éœ€è¦å•ç‹¬ä¿å­˜ä¸ºä¸€ä¸ªæ–‡ä»¶ï¼š
For the CSV and JSON formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the DatasetDict objectã€‚
And thatâ€™s it for our excursion into data wrangling with ğŸ¤— Datasets! 

å¦‚æœä½ æƒ³èŠ‚çœç©ºé—´ï¼Œé‚£ä¹ˆå¯ä»¥åœ¨è§£å‹ä¹‹ååˆ é™¤å‹ç¼©æ–‡ä»¶ï¼šdownload_config

```python
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```
è¿™é‡Œçš„ split è¡¨ç¤ºåªåŠ è½½ 'train' è¿™éƒ¨åˆ†çš„æ•°æ®ã€‚  
streaming=True å¾—åˆ°çš„åˆ™æ˜¯ä¸€ä¸ª è¿­ä»£å™¨ã€‚
å¯ä»¥é€šè¿‡æŒ‡å®šbatched æ¥ä½¿ç”¨batchã€‚
è¿­ä»£å™¨ä¹Ÿæœ‰ .map() å‡½æ•°

shuffle æµå¼æ•°æ®ï¼š
```python
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```
è¿™æ˜¯åªshuffle 10_000 ä¸ªæ•°æ®ï¼Œbatchçš„å¤§å°è‡ªå·±æŒ‡å®šã€‚ shuffleä¹‹åï¼Œè¿˜æ˜¯æŒ‰ç…§è¿­ä»£å™¨çš„æ–¹å¼è¿›è¡Œè®¿é—®ã€‚

```python
# Skip the first 1,000 examples and include the rest in the training set
train_dataset = shuffled_dataset.skip(1000)
# Take the first 1,000 examples for the validation set
validation_dataset = shuffled_dataset.take(1000)
```
ä¹Ÿæœ‰ä¸€äº›é€‰æ‹©å’Œä¸¢å¼ƒçš„å‡½æ•°ã€‚

æœ€åè¿˜è¯´åˆ°å¯ä»¥é€šè¿‡ interleave_datasets() å‡½æ•°æ¥ç»„åˆå¤šä¸ªæ•°æ®é›†ã€‚

å¾ˆé€—ï¼Œå±…ç„¶é€šè¿‡è¿™ç§æ–¹å¼æ¥å®ç°æ•°æ®ç±»å‹çš„è½¬æ¢ï¼š
issues_dataset.set_format("pandas")
df = issues_dataset[:] # è½¬ä¸ºpandasæ•°æ®ç±»å‹

pandas çš„ df.explode() å‡½æ•°èƒ½å®ç°å°†åŒ…å«å¤šä¸ªå…ƒç´ çš„å•å…ƒæ ¼å±•å¼€å¹¶ä¸”è†¨èƒ€ä¸ºå¤šè¡Œã€‚å…¶ä»–åˆ—çš„ä¿¡æ¯å°†è¢«å¤åˆ¶åˆ°å¤šè¡Œä¸­ã€‚

è¿™é‡Œè¿˜æœ‰ä¸ªç¥æ“ä½œï¼š
ç›´æ¥å°†æ¨¡å‹æ”¾åœ¨ mapå‡½æ•°ä¸­ï¼š
```python
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```
get_embeddings æ˜¯è‡ªå®šä¹‰çš„å‡½æ•°ï¼ŒåŒ…å«äº†ä¸€ä¸ªæ¨¡å‹ã€‚
ä½†æ˜¯è¿™é‡Œè¿˜æ˜¯ä¸èƒ½åˆ©ç”¨æ¨¡å‹çš„batchèƒ½åŠ›ã€‚å› ä¸ºmap çš„batchå®é™…ä¸Šæ˜¯list comprehension.

Datasets è‡ªå¸¦ä¸€ä¸ªæ•°æ®æ£€ç´¢çš„åŠŸèƒ½ï¼š
Datasets called a FAISS index. FAISS (short for Facebook AI Similarity Search) is a library that provides efficient algorithms to quickly search and cluster embedding vectors.
å¯ä»¥å®ç°vector storeçš„åŠŸèƒ½ã€‚

# Tokenizer

# Evaluation
æ³¨æ„ç”¨äºè®­ç»ƒçš„ loss function å’Œevaluation å‡½æ•°æ˜¯ä¸ä¸€æ ·çš„ã€‚
å¹¶ä¸”äº¤å‰ç†µå¯ä»¥ç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ã€‚è‡ªç„¶è¯­è¨€å¤„ç†ä¸­é¢„æµ‹ä¸‹ä¸€ä¸ªtokenå°±æ˜¯å¤šåˆ†ç±»ä»»åŠ¡ã€‚
https://zhuanlan.zhihu.com/p/56638625


# Metrics
éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒMetricsçš„è¾“å…¥æ˜¯æ–‡æœ¬ï¼Œå¹¶ä¸”æ˜¯è¡¡é‡æ¨¡å‹çš„è¾“å‡ºçš„æ€§èƒ½çš„ã€‚è€Œä¸æ˜¯æŸå¤±å‡½æ•°ã€‚  
æ³¨æ„å…¶ä¸­çš„ Perplexity æ˜¯ç›´æ¥ä½¿ç”¨ç›®æ ‡tokençš„æ¦‚ç‡æ¥è¿›è¡Œè¡¡é‡çš„ã€‚è¿™ä¹Ÿä¸æ˜¯æŸå¤±å‡½æ•°ã€‚ 
https://blog.csdn.net/codename_cys/article/details/108654792

## ppl (perplexity)
é©¬å°”å¯å¤«å‡è®¾ã€‚ç”¨æ¯ä¸ªå•è¯è¢«é¢„æµ‹çš„æ¦‚ç‡æ¥è®¡ç®—æ–‡æœ¬çš„è¿è´¯ç¨‹åº¦ã€‚ 

## bleu (bilingual evaluation understudy)
æ ¸å¿ƒæ€æƒ³éƒ½æ˜¯æ¯”è¾ƒç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬é—´çš„å­—ç¬¦ä¸²é‡åˆåº¦ã€‚  
è¿™ä¸ªæŒ‡æ ‡æ˜¯åŸºäº n-gram çš„ã€‚ 
è¡¡é‡çš„æ˜¯ç”Ÿæˆçš„æ–‡æœ¬â€œå‡»ä¸­â€å‚è€ƒæ–‡æœ¬çš„æ¦‚ç‡ã€‚  

## rouge 
è¿™ä¹Ÿæ˜¯åŸºäºn-gram çš„ã€‚åªæ˜¯è¡¡é‡çš„å†…å®¹ä¸ä¸€æ ·ã€‚  
è¡¡é‡çš„æ˜¯å‚è€ƒæ–‡æœ¬â€œå‡»ä¸­â€ç”Ÿæˆæ–‡æœ¬çš„æ¦‚ç‡ã€‚  

## bleurt
è¿™ç§æ–¹æ³•æ¯”è¾ƒ fancyï¼Œå®ƒé€šè¿‡ä¸€ä¸ªæ¨¡å‹æ¥å¯¹ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬è¿›è¡Œè¯„ä¼°ï¼Œè®¡ç®—æ–‡æœ¬ä¹‹é—´çš„è·ç¦»ã€‚  

# Huggingface Metrics

https://huggingface.co/docs/datasets/en/how_to_metrics
è¿™ä¸¤ä¸ªæ–‡ç« è®²è§£äº†æ€ä¹ˆåŠ è½½ metricsï¼Œæ€ä¹ˆä¸‹è½½metricsï¼Œä»¥åŠæ€ä¹ˆå®šä¹‰å’ŒåŠ è½½è‡ªå·±çš„metricsã€‚  
å¹¶ä¸”è¿™é‡ŒæŒ‡å®šäº†ä¸€ä¸ªä½¿ç”¨ metrics çš„æ¥å£ï¼š
```python
import datasets
metric = datasets.load_metric('my_metric')
for model_input, gold_references in evaluation_dataset:
    model_predictions = model(model_inputs)
    metric.add_batch(predictions=model_predictions, references=gold_references)
final_score = metric.compute()
```
æ¥å£æ˜¯ï¼šç›´æ¥å°†æ¨¡å‹çš„è¾“å‡º å’Œ æ•°æ®ä¸­çš„ label è¾“å…¥ç»™ metricï¼Œè‡³äºæ€ä¹ˆè®¡ç®—ï¼Œä»¥åŠéœ€è¦ä»€ä¹ˆæ ·çš„åå¤„ç†ï¼Œéƒ½äº¤ç»™metric è‡ªå·±å®Œæˆã€‚  
è€Œåœ¨Huggingfaceä¸­ æ¨¡å‹çš„è¾“å‡º é»˜è®¤æ˜¯ logitsã€‚  
`é‚£ä¹ˆç°åœ¨å°±æ˜¯éœ€è¦ç¡®å®šï¼Œpredictions and references é•¿ä»€ä¹ˆæ ·ã€‚ éœ€è¦çœ‹ä¾‹å­ã€‚`  


ä»è¿™ä¸ªæ–‡ç« æ¥çœ‹ï¼Œå…¶å® metrics çš„è®¡ç®—æ–¹æ³•éœ€è¦ä¸¤ä¸ªè¾“å…¥å‚æ•° predictions and referencesã€‚  
å…¶å®å¹¶æ²¡æœ‰è§„å®šæ˜¯ logits è¿˜æ˜¯ textã€‚  
æ‰€ä»¥å¯èƒ½è¿˜æ˜¯æ¯”è¾ƒçµæ´»ã€‚  
ä½†æ˜¯é»˜è®¤æƒ…å†µä¸‹å¯èƒ½è¿˜æ˜¯æ¯”è¾ƒ predictions è¿˜æ˜¯ logitsã€‚ ä½†æ˜¯references å°±ä¸ç¡®å®šäº†ã€‚å› ä¸ºä¸åŒçš„ä»»åŠ¡çš„label ä¸ä¸€æ ·ã€‚  

## Inputs of metrics
text, logits, dictionary, numerics, etc.

## Choosing a metric for your task
https://huggingface.co/docs/evaluate/en/choosing_a_metric
metrics çš„æ¥æºä¸»è¦æœ‰ä¸‰ç§ï¼š
1. ä¸€èˆ¬çš„accuracyã€precisionã€recallç­‰ã€‚
2. æ•°æ®é›†è‡ªå®šä¹‰çš„ metricsï¼Œé‚£ä¹ˆè¿™å°±éœ€è¦çœ‹æ•°æ®é›†çš„æºç ã€‚
3. ä»»åŠ¡è‡ªå®šä¹‰çš„metricsï¼Œè¿™å°±éœ€è¦çœ‹å…¶å¯¹åº”çš„å®šä¹‰å’Œæºç äº†ã€‚

## Metrics
ä»å‡ ä¸ª metrics çš„æºç æ¥çœ‹ï¼Œä¸åŒçš„metricsæ¥å—çš„è¾“å…¥ä¸åŒã€‚æœ‰äº›æ˜¯ textï¼Œæœ‰äº›æ˜¯å­—å…¸ã€‚  
å¹¶ä¸”metricsä¸­ä¼šæœ‰ä¸€äº›æ•°æ®é¢„å¤„ç†çš„ä»£ç ã€‚  

é‚£ä¹ˆHuggingface æä¾›çš„ meteicså‘¢ï¼Ÿ  
huggingfaceä¸Šçš„æ¨¡å‹çš„è¾“å‡ºéƒ½æ˜¯åŒ…å«åœ¨ä¸€ä¸ªè¾“å‡ºç±»å‹ä¸­çš„ï¼Œè¿™ä¸ªç±»å‹æ˜¯æ ¹æ®ä¸åŒçš„ä»»åŠ¡è€Œä¸åŒï¼Œä½†æ˜¯éƒ½æ˜¯å­—å…¸ã€‚  
å¹¶ä¸”ä¸€èˆ¬æ¥è¯´ï¼Œæ¨¡å‹çš„ç›´æ¥è¾“å‡ºéƒ½æ˜¯logitsï¼ˆåŒ…å«åœ¨ è¿”å›ç±»å‹ä¸­ï¼‰  

Metricsçš„ä½¿ç”¨æ–¹å¼å¾ˆå¤šã€‚å¯ä»¥æ”¾åœ¨ Trainerä¸­ä½¿ç”¨ã€‚å¯ä»¥æ”¾åœ¨ evaluatorä¸­ä½¿ç”¨ã€‚ä¹Ÿå¯ä»¥æ‹¿åˆ°æ¨¡å‹çš„è¾“å‡ºä¹‹åï¼Œå•ç‹¬ä½¿ç”¨ã€‚å®ƒå°±æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œéšä¾¿ä½ åœ¨å“ªé‡Œè°ƒç”¨ã€‚

å¹¶ä¸”metricçš„ä½¿ç”¨å¾€å¾€æ˜¯å’Œ Evaluator ç»“åˆèµ·æ¥çš„ï¼š  
https://huggingface.co/docs/evaluate/en/base_evaluator

æ€»ç»“ï¼š  
å…³äº metrics çš„ä½¿ç”¨æ–¹æ³•ï¼š ä¸‰ç§ä½¿ç”¨æ–¹æ³•ã€‚ä½†æ˜¯å¦‚æœè¦æ”¾åœ¨Trainerä¸­è‡ªåŠ¨è°ƒç”¨çš„è¯ï¼Œå¾€å¾€éœ€è¦è¿›è¡Œ wrappingã€‚  
huggingface å†…éƒ¨é»˜è®¤çš„è°ƒç”¨æ–¹æ³•æ˜¯ç›´æ¥å°†æ¨¡å‹è¾“å‡ºå’Œæ•°æ®é›†ä¸­çš„labelç›´æ¥è¾“å…¥ç»™metricså‡½æ•°ï¼š  
```python
for model_input, gold_references in evaluation_dataset:
    model_predictions = model(model_inputs)
    metric.add_batch(predictions=model_predictions, references=gold_references)
    final_score = metric.compute()
``` 
ä½†æ˜¯æ³¨æ„ï¼Œåœ¨ Trainer ä¸­ï¼Œå¹¶ä¸æ˜¯ç›´æ¥å°†æ¨¡å‹çš„è¾“å‡ºè¾“å…¥ç»™ metricsçš„ã€‚ è€Œæ˜¯å°†æ¨¡å‹çš„è¾“å‡ºä¸­çš„ logitsæ‹¿å‡ºæ¥ï¼ŒåŠ ä¸Šä»æ•°æ®ä¸­æå–çš„labelsï¼Œä¸€èµ·è¾“å…¥ç»™metricsã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯èƒ½åœ¨ä¸åŒçš„åœ°æ–¹ï¼Œé»˜è®¤çš„è§„åˆ™ä¸ä¸€æ ·ã€‚éœ€è¦çœ‹æºç ç¡®å®šã€‚  
æˆ–è€…å¦ä¸€ä¸ªç®€å•çš„åŠæ³•æ˜¯ï¼Œè‡ªå·±å†™ä¸€ä¸ªmetricså‡½æ•°ï¼Œç„¶ååœ¨è¿™ä¸ªå‡½æ•°ä¸­æ‰“å°è¾“å…¥å‚æ•°ã€‚ç„¶åå°±çŸ¥é“åº”è¯¥æ€ä¹ˆå¤„ç†äº†ã€‚  

### Accuracy
https://huggingface.co/spaces/evaluate-metric/accuracy 
```python
>>> accuracy_metric = evaluate.load("accuracy")
>>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])
>>> print(results)
{'accuracy': 0.5}
```
è¿™æ˜¯å‡†ç¡®ç‡çš„ç”¨æ³•ã€‚ä½†æ˜¯æˆ‘ä»¬çŸ¥é“æ¨¡å‹çš„è¾“å‡ºå¹¶ä¸æ˜¯è¿™ç§æ ¼å¼çš„ã€‚å¹¶ä¸”æœ‰æ—¶å€™å¾—åˆ°çš„æ˜¯æ–‡æœ¬ã€‚è€Œä¸æ˜¯æ•°å­—ã€‚  
è¿™æ—¶å€™å°±éœ€è¦è¿›è¡Œé¢„å¤„ç†ã€‚æˆ–è€…å¯¹æ–‡æœ¬è¿›è¡Œæ•°å­—åŒ–ï¼Œæ¯”å¦‚tokenization æˆ–è€…ç›´æ¥æ¯”è¾ƒå­—ç¬¦ä¸²ã€‚å°±å’Œ accuracy æœ¬èº«çš„ä»£ç ä¸€æ ·ï¼Œç›´æ¥æ¯”è¾ƒå€¼ã€‚  

å®é™…æ¡ˆä¾‹ï¼š
```python
import evaluate
metric = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```
åœ¨è¿™é‡Œæˆ‘ä»¬å¯¹ accuracy è¿›è¡Œäº†åŒ…è£…ã€‚å› ä¸ºä¸€èˆ¬æ¥è¯´æ¨¡å‹çš„è¾“å‡ºæ˜¯ logitsï¼Œæˆ–è€…åŒ…å«äº† logitsçš„ å­—å…¸ã€‚  
è¿™é‡Œçš„é¢„å¤„ç†æ˜¯å¯¹logits è¿›è¡Œ greedy selectionã€‚è·å¾—æ ‡ç­¾ã€‚  

### Perplexity
https://huggingface.co/spaces/evaluate-metric/perplexity
å®ƒè¿™é‡Œæä¾›çš„ä¾‹å­å¾ˆå¥‡æ€ªã€‚  
metric åœ¨ä½¿ç”¨æ—¶éœ€è¦æŒ‡å®š model_idï¼Œä½†æ˜¯å¹¶æ²¡æœ‰è¯´æ”¯æŒä¼ å…¥æ¨¡å‹å¯¹è±¡ã€‚  
é‚£ä¹ˆå°±æ„å‘³ç€ï¼Œä½ è¦æ˜¯æƒ³æµ‹è¯•è‡ªå·±çš„æ¨¡å‹ï¼Œä½ å°±è¦éœ€è¦æ›¿æ¢æˆè‡ªå·±çš„æ¨¡å‹çš„åå­—ã€‚æˆ–è€…å°†è‡ªå·±çš„æ¨¡å‹ä¼ åˆ°huggingfaceä¸Šã€‚  
å¹¶ä¸”é‰´äºè¿™ç§è‡ªå·±åŠ è½½æ¨¡å‹çš„è®¾å®šã€‚å®ƒä¸é€‚ç”¨äºTrainerã€‚ 

### Rouge
https://huggingface.co/spaces/evaluate-metric/rouge
```python
>>> rouge = evaluate.load('rouge')
>>> predictions = ["hello there", "general kenobi"]
>>> references = ["hello there", "general kenobi"]
>>> results = rouge.compute(predictions=predictions,
...                         references=references)
>>> print(results)
{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}
```
å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„è¾“å…¥ä¹Ÿæ˜¯textã€‚  


### GLUE
https://github.com/huggingface/datasets/blob/main/metrics/glue/glue.py
ä» GLUE çš„æºç æ¥çœ‹ï¼Œå…¶å®å¯¹äº accuracy ä¹‹ç±»çš„ metricsï¼Œ å®ƒä»¬çš„å®ç°è¿˜æ˜¯å¾ˆç®€å•æš´åŠ›çš„ã€‚  

huggingface example:
https://huggingface.co/spaces/evaluate-metric/bleu
```python
>>> predictions = ["hello there general kenobi","foo bar foobar"]
>>> references = [
...     ["hello there general kenobi"],
...     ["foo bar foobar"]
... ]
>>> bleu = evaluate.load("bleu")
>>> results = bleu.compute(predictions=predictions, references=references)
>>> print(results)
{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 7, 'reference_leng}
# results = bleu.compute(predictions=predictions, references=references, tokenizer=word_tokenize)

```
å¯ä»¥çœ‹åˆ°åœ¨huggingfaceä¸­å®ƒçš„è¾“å…¥ä¹Ÿè¿˜æ˜¯textã€‚è¿˜å¯ä»¥å°† tokenizer ä¼ ç»™ metricã€‚


###  SacreBLEU metric
https://huggingface.co/spaces/evaluate-metric/sacrebleu
è¿™é‡Œç»™å‡ºäº†ä¸€ä¸ªä¾‹å­ï¼š
```python
>>> predictions = ["hello there general kenobi", "foo bar foobar"]
>>> references = [["hello there general kenobi", "hello there !"],
...                 ["foo bar foobar", "foo bar foobar"]]
>>> sacrebleu = evaluate.load("sacrebleu")
>>> results = sacrebleu.compute(predictions=predictions, 
...                             references=references)
>>> print(list(results.keys()))
['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']
>>> print(round(results["score"], 1))
100.0
```
æ³¨æ„ï¼Œè¿™é‡Œ metric çš„è¾“å…¥æ˜¯textã€‚ä½†æ˜¯åœ¨è§£é‡Šä¸­ï¼Œä»–åˆè¯´çš„æ˜¯ï¼šâ€˜predictions (list of str): list of translations to score. Each translation should be tokenized into a list of tokens.â€™
ä¹Ÿå°±æ˜¯idsã€‚ ä½†è¿™ä¹Ÿä¸æ˜¯æ¨¡å‹çš„logitsã€‚  


### SQuAD metric
https://github.com/huggingface/datasets/blob/main/metrics/squad/squad.py
è¿™é‡Œæä¾›äº†ä¸€ä¸ªå‚è€ƒä¾‹å­ã€‚  
```python
>>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
>>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
>>> squad_metric = datasets.load_metric("squad")
>>> results = squad_metric.compute(predictions=predictions, references=references)
>>> print(results)
{'exact_match': 100.0, 'f1': 100.0}
```
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œmetricçš„è¾“å…¥ç«Ÿç„¶åˆæ˜¯ å­—å…¸ã€‚ è¿™ä¸ªå¾ˆç¬¦åˆ huggingface model çš„è¾“å‡ºã€‚  
åŒæ—¶å¯ä»¥çœ‹åˆ°çš„æ˜¯ï¼Œåœ¨metric é‡Œé¢ï¼Œå¯¹è¾“å…¥çš„å­—å…¸è¿›è¡Œäº†è§£æã€‚ç„¶åæ‰æ˜¯è®¡ç®—ã€‚  


### BLEURT metric
https://github.com/huggingface/datasets/blob/main/metrics/bleurt/bleurt.py

```python
>>> predictions = ["hello there", "general kenobi"]
>>> references = ["hello there", "general kenobi"]
>>> bleurt = datasets.load_metric("bleurt")
>>> results = bleurt.compute(predictions=predictions, references=references)
>>> print([round(v, 2) for v in results["scores"]])
[1.03, 1.04]
```
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œmetric æ¥å—çš„è¾“å…¥æ˜¯textã€‚  


å®å¹³å‡ï¼ˆMarco Averagedï¼‰:
å¯¹æ‰€æœ‰ç±»åˆ«çš„æ¯ä¸€ä¸ªç»Ÿè®¡æŒ‡æ ‡å€¼çš„ç®—æ•°å¹³å‡å€¼ï¼Œåˆ†åˆ«ç§°ä¸ºå®ç²¾ç¡®ç‡ï¼ˆMacro-Precisionï¼‰ ï¼Œå®å¬å›ç‡ï¼ˆMacro-Recallï¼‰ï¼Œå®Få€¼ï¼ˆMacro-F Scoreï¼‰

macro ËˆmÃ¦krÉ™ÊŠ
1. [ADJ]You use macro to indicate that something relates to a general area, rather than being detailed or specific. å®è§‚çš„











